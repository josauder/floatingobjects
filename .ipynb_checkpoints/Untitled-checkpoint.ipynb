{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2865b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: trainloss 0.1914, valloss 0.0683, precision 0.55, recall 0.54, fscore 0.54, kappa 0.08, auroc 0.72\n",
      "\n",
      "epoch 2: trainloss 0.1502, valloss 0.0720, precision 0.55, recall 0.56, fscore 0.56, kappa 0.11, auroc 0.73\n",
      "\n",
      "epoch 3: trainloss 0.1453, valloss 0.0630, precision 0.56, recall 0.54, fscore 0.55, kappa 0.09, auroc 0.82\n",
      "\n",
      "epoch 4: trainloss 0.1394, valloss 0.0671, precision 0.55, recall 0.54, fscore 0.54, kappa 0.09, auroc 0.74\n",
      "\n",
      "epoch 5: trainloss 0.1377, valloss 0.0625, precision 0.56, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.75\n",
      "\n",
      "epoch 6: trainloss 0.1359, valloss 0.0625, precision 0.55, recall 0.52, fscore 0.53, kappa 0.05, auroc 0.77\n",
      "\n",
      "epoch 7: trainloss 0.1339, valloss 0.0612, precision 0.57, recall 0.52, fscore 0.53, kappa 0.06, auroc 0.80\n",
      "\n",
      "epoch 8: trainloss 0.1321, valloss 0.0604, precision 0.57, recall 0.51, fscore 0.51, kappa 0.03, auroc 0.79\n",
      "\n",
      "epoch 9: trainloss 0.1315, valloss 0.0606, precision 0.58, recall 0.54, fscore 0.55, kappa 0.11, auroc 0.81\n",
      "\n",
      "epoch 10: trainloss 0.1259, valloss 0.0619, precision 0.59, recall 0.51, fscore 0.52, kappa 0.03, auroc 0.78\n",
      "\n",
      "epoch 11: trainloss 0.1249, valloss 0.0607, precision 0.53, recall 0.50, fscore 0.50, kappa 0.01, auroc 0.80\n",
      "\n",
      "epoch 12: trainloss 0.1223, valloss 0.0651, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.78\n",
      "\n",
      "epoch 13: trainloss 0.1198, valloss 0.0613, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.82\n",
      "\n",
      "epoch 14: trainloss 0.1202, valloss 0.0617, precision 0.56, recall 0.52, fscore 0.53, kappa 0.06, auroc 0.75\n",
      "\n",
      "epoch 15: trainloss 0.1178, valloss 0.0573, precision 0.58, recall 0.54, fscore 0.55, kappa 0.10, auroc 0.85\n",
      "\n",
      "epoch 16: trainloss 0.1176, valloss 0.0599, precision 0.56, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.79\n",
      "\n",
      "epoch 17: trainloss 0.1170, valloss 0.0589, precision 0.57, recall 0.56, fscore 0.56, kappa 0.13, auroc 0.83\n",
      "\n",
      "epoch 18: trainloss 0.1160, valloss 0.0604, precision 0.57, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.84\n",
      "\n",
      "epoch 19: trainloss 0.1155, valloss 0.0636, precision 0.56, recall 0.55, fscore 0.56, kappa 0.11, auroc 0.83\n",
      "\n",
      "epoch 20: trainloss 0.1141, valloss 0.0639, precision 0.55, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.80\n",
      "\n",
      "epoch 21: trainloss 0.1129, valloss 0.0619, precision 0.56, recall 0.54, fscore 0.54, kappa 0.09, auroc 0.83\n",
      "\n",
      "epoch 22: trainloss 0.1132, valloss 0.0606, precision 0.57, recall 0.58, fscore 0.58, kappa 0.15, auroc 0.84\n",
      "\n",
      "epoch 23: trainloss 0.1116, valloss 0.0597, precision 0.58, recall 0.54, fscore 0.55, kappa 0.10, auroc 0.81\n",
      "\n",
      "epoch 24: trainloss 0.1121, valloss 0.0580, precision 0.57, recall 0.55, fscore 0.56, kappa 0.12, auroc 0.86\n",
      "\n",
      "epoch 25: trainloss 0.1118, valloss 0.0578, precision 0.58, recall 0.54, fscore 0.55, kappa 0.11, auroc 0.85\n",
      "\n",
      "epoch 26: trainloss 0.1110, valloss 0.0579, precision 0.57, recall 0.58, fscore 0.58, kappa 0.15, auroc 0.86\n",
      "\n",
      "epoch 27: trainloss 0.1103, valloss 0.0548, precision 0.58, recall 0.55, fscore 0.56, kappa 0.12, auroc 0.86\n",
      "\n",
      "epoch 28: trainloss 0.1079, valloss 0.0582, precision 0.56, recall 0.55, fscore 0.56, kappa 0.11, auroc 0.88\n",
      "\n",
      "epoch 29: trainloss 0.1079, valloss 0.0543, precision 0.58, recall 0.56, fscore 0.57, kappa 0.13, auroc 0.88\n",
      "\n",
      "epoch 30: trainloss 0.1070, valloss 0.0528, precision 0.59, recall 0.54, fscore 0.56, kappa 0.12, auroc 0.88\n",
      "\n",
      "epoch 31: trainloss 0.1067, valloss 0.0551, precision 0.58, recall 0.56, fscore 0.57, kappa 0.13, auroc 0.87\n",
      "\n",
      "epoch 32: trainloss 0.1069, valloss 0.0538, precision 0.58, recall 0.55, fscore 0.56, kappa 0.13, auroc 0.87\n",
      "\n",
      "epoch 33: trainloss 0.1072, valloss 0.0541, precision 0.58, recall 0.55, fscore 0.56, kappa 0.13, auroc 0.88\n",
      "\n",
      "epoch 34: trainloss 0.1061, valloss 0.0525, precision 0.60, recall 0.59, fscore 0.59, kappa 0.18, auroc 0.89\n",
      "\n",
      "epoch 35: trainloss 0.1046, valloss 0.0561, precision 0.58, recall 0.59, fscore 0.59, kappa 0.17, auroc 0.87\n",
      "\n",
      "epoch 36: trainloss 0.1039, valloss 0.0529, precision 0.59, recall 0.59, fscore 0.59, kappa 0.18, auroc 0.90\n",
      "\n",
      "epoch 37: trainloss 0.1038, valloss 0.0526, precision 0.60, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.88\n",
      "\n",
      "epoch 38: trainloss 0.1036, valloss 0.0538, precision 0.59, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.88\n",
      "\n",
      "epoch 39: trainloss 0.1036, valloss 0.0513, precision 0.60, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.90\n",
      "\n",
      "epoch 40: trainloss 0.1028, valloss 0.0539, precision 0.59, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.88\n",
      "\n",
      "epoch 41: trainloss 0.1023, valloss 0.0522, precision 0.60, recall 0.56, fscore 0.58, kappa 0.16, auroc 0.89\n",
      "\n",
      "epoch 42: trainloss 0.1029, valloss 0.0554, precision 0.61, recall 0.53, fscore 0.55, kappa 0.10, auroc 0.89\n",
      "\n",
      "epoch 43: trainloss 0.1018, valloss 0.0548, precision 0.61, recall 0.55, fscore 0.56, kappa 0.13, auroc 0.88\n",
      "\n",
      "epoch 44: trainloss 0.1009, valloss 0.0551, precision 0.58, recall 0.56, fscore 0.57, kappa 0.13, auroc 0.89\n",
      "\n",
      "epoch 45: trainloss 0.1018, valloss 0.0506, precision 0.60, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.90\n",
      "\n",
      "epoch 46: trainloss 0.0996, valloss 0.0584, precision 0.58, recall 0.56, fscore 0.57, kappa 0.13, auroc 0.87\n",
      "\n",
      "epoch 47: trainloss 0.1009, valloss 0.0503, precision 0.62, recall 0.56, fscore 0.57, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 48: trainloss 0.1005, valloss 0.0513, precision 0.60, recall 0.55, fscore 0.57, kappa 0.14, auroc 0.89\n",
      "\n",
      "epoch 49: trainloss 0.0998, valloss 0.0498, precision 0.61, recall 0.57, fscore 0.58, kappa 0.16, auroc 0.91\n",
      "\n",
      "epoch 50: trainloss 0.1009, valloss 0.0542, precision 0.61, recall 0.56, fscore 0.58, kappa 0.16, auroc 0.89\n",
      "\n",
      "[[0.1914 0.1502 0.1453 0.1394 0.1377 0.1359 0.1339 0.1321 0.1315 0.1259\n",
      "  0.1249 0.1223 0.1198 0.1202 0.1178 0.1176 0.117  0.116  0.1155 0.1141\n",
      "  0.1129 0.1132 0.1116 0.1121 0.1118 0.111  0.1103 0.1079 0.1079 0.107\n",
      "  0.1067 0.1069 0.1072 0.1061 0.1046 0.1039 0.1038 0.1036 0.1036 0.1028\n",
      "  0.1023 0.1029 0.1018 0.1009 0.1018 0.0996 0.1009 0.1005 0.0998 0.1009]\n",
      " [0.0683 0.072  0.063  0.0671 0.0625 0.0625 0.0612 0.0604 0.0606 0.0619\n",
      "  0.0607 0.0651 0.0613 0.0617 0.0573 0.0599 0.0589 0.0604 0.0636 0.0639\n",
      "  0.0619 0.0606 0.0597 0.058  0.0578 0.0579 0.0548 0.0582 0.0543 0.0528\n",
      "  0.0551 0.0538 0.0541 0.0525 0.0561 0.0529 0.0526 0.0538 0.0513 0.0539\n",
      "  0.0522 0.0554 0.0548 0.0551 0.0506 0.0584 0.0503 0.0513 0.0498 0.0542]\n",
      " [0.55   0.55   0.56   0.55   0.56   0.55   0.57   0.57   0.58   0.59\n",
      "  0.53   0.56   0.56   0.56   0.58   0.56   0.57   0.57   0.56   0.55\n",
      "  0.56   0.57   0.58   0.57   0.58   0.57   0.58   0.56   0.58   0.59\n",
      "  0.58   0.58   0.58   0.6    0.58   0.59   0.6    0.59   0.6    0.59\n",
      "  0.6    0.61   0.61   0.58   0.6    0.58   0.62   0.6    0.61   0.61  ]\n",
      " [0.54   0.56   0.54   0.54   0.53   0.52   0.52   0.51   0.54   0.51\n",
      "  0.5    0.53   0.53   0.52   0.54   0.51   0.56   0.53   0.55   0.51\n",
      "  0.54   0.58   0.54   0.55   0.54   0.58   0.55   0.55   0.56   0.54\n",
      "  0.56   0.55   0.55   0.59   0.59   0.59   0.56   0.56   0.56   0.56\n",
      "  0.56   0.53   0.55   0.56   0.56   0.56   0.56   0.55   0.57   0.56  ]\n",
      " [0.54   0.56   0.55   0.54   0.53   0.53   0.53   0.51   0.55   0.52\n",
      "  0.5    0.54   0.54   0.53   0.55   0.52   0.56   0.54   0.56   0.52\n",
      "  0.54   0.58   0.55   0.56   0.55   0.58   0.56   0.56   0.57   0.56\n",
      "  0.57   0.56   0.56   0.59   0.59   0.59   0.57   0.57   0.57   0.57\n",
      "  0.58   0.55   0.56   0.57   0.57   0.57   0.57   0.57   0.58   0.58  ]\n",
      " [0.08   0.11   0.09   0.09   0.07   0.05   0.06   0.03   0.11   0.03\n",
      "  0.01   0.08   0.08   0.06   0.1    0.04   0.13   0.08   0.11   0.04\n",
      "  0.09   0.15   0.1    0.12   0.11   0.15   0.12   0.11   0.13   0.12\n",
      "  0.13   0.13   0.13   0.18   0.17   0.18   0.14   0.14   0.14   0.14\n",
      "  0.16   0.1    0.13   0.13   0.14   0.13   0.15   0.14   0.16   0.16  ]\n",
      " [0.72   0.73   0.82   0.74   0.75   0.77   0.8    0.79   0.81   0.78\n",
      "  0.8    0.78   0.82   0.75   0.85   0.79   0.83   0.84   0.83   0.8\n",
      "  0.83   0.84   0.81   0.86   0.85   0.86   0.86   0.88   0.88   0.88\n",
      "  0.87   0.87   0.88   0.89   0.87   0.9    0.88   0.88   0.9    0.88\n",
      "  0.89   0.89   0.88   0.89   0.9    0.87   0.9    0.89   0.91   0.89  ]]\n",
      "epoch 1: trainloss 0.2015, valloss 0.0706, precision 0.59, recall 0.51, fscore 0.51, kappa 0.03, auroc 0.73\n",
      "\n",
      "epoch 2: trainloss 0.1653, valloss 0.0699, precision 0.60, recall 0.52, fscore 0.53, kappa 0.07, auroc 0.77\n",
      "\n",
      "epoch 3: trainloss 0.1638, valloss 0.0731, precision 0.59, recall 0.52, fscore 0.52, kappa 0.05, auroc 0.73\n",
      "\n",
      "epoch 4: trainloss 0.1614, valloss 0.0752, precision 0.55, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.72\n",
      "\n",
      "epoch 5: trainloss 0.1611, valloss 0.0713, precision 0.60, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.72\n",
      "\n",
      "epoch 6: trainloss 0.1572, valloss 0.0699, precision 0.60, recall 0.52, fscore 0.52, kappa 0.05, auroc 0.73\n",
      "\n",
      "epoch 7: trainloss 0.1565, valloss 0.0714, precision 0.58, recall 0.53, fscore 0.53, kappa 0.06, auroc 0.80\n",
      "\n",
      "epoch 8: trainloss 0.1555, valloss 0.0701, precision 0.61, recall 0.51, fscore 0.52, kappa 0.05, auroc 0.74\n",
      "\n",
      "epoch 9: trainloss 0.1557, valloss 0.0683, precision 0.61, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.78\n",
      "\n",
      "epoch 10: trainloss 0.1539, valloss 0.0685, precision 0.58, recall 0.51, fscore 0.51, kappa 0.02, auroc 0.74\n",
      "\n",
      "epoch 11: trainloss 0.1529, valloss 0.0711, precision 0.60, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.79\n",
      "\n",
      "epoch 12: trainloss 0.1546, valloss 0.0732, precision 0.59, recall 0.51, fscore 0.51, kappa 0.03, auroc 0.75\n",
      "\n",
      "epoch 13: trainloss 0.1534, valloss 0.0717, precision 0.56, recall 0.52, fscore 0.52, kappa 0.05, auroc 0.81\n",
      "\n",
      "epoch 14: trainloss 0.1523, valloss 0.0696, precision 0.57, recall 0.52, fscore 0.53, kappa 0.06, auroc 0.81\n",
      "\n",
      "epoch 15: trainloss 0.1538, valloss 0.0782, precision 0.55, recall 0.52, fscore 0.52, kappa 0.05, auroc 0.77\n",
      "\n",
      "epoch 16: trainloss 0.1512, valloss 0.0731, precision 0.55, recall 0.54, fscore 0.54, kappa 0.08, auroc 0.82\n",
      "\n",
      "epoch 17: trainloss 0.1454, valloss 0.0645, precision 0.60, recall 0.54, fscore 0.55, kappa 0.10, auroc 0.83\n",
      "\n",
      "epoch 18: trainloss 0.1429, valloss 0.0624, precision 0.63, recall 0.55, fscore 0.57, kappa 0.13, auroc 0.83\n",
      "\n",
      "epoch 19: trainloss 0.1376, valloss 0.0633, precision 0.58, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.85\n",
      "\n",
      "epoch 20: trainloss 0.1368, valloss 0.0616, precision 0.63, recall 0.57, fscore 0.59, kappa 0.17, auroc 0.84\n",
      "\n",
      "epoch 21: trainloss 0.1352, valloss 0.0620, precision 0.63, recall 0.58, fscore 0.59, kappa 0.18, auroc 0.85\n",
      "\n",
      "epoch 22: trainloss 0.1345, valloss 0.0607, precision 0.63, recall 0.59, fscore 0.60, kappa 0.21, auroc 0.86\n",
      "\n",
      "epoch 23: trainloss 0.1315, valloss 0.0564, precision 0.64, recall 0.61, fscore 0.62, kappa 0.24, auroc 0.89\n",
      "\n",
      "epoch 24: trainloss 0.1310, valloss 0.0620, precision 0.62, recall 0.60, fscore 0.61, kappa 0.21, auroc 0.85\n",
      "\n",
      "epoch 25: trainloss 0.1295, valloss 0.0597, precision 0.62, recall 0.60, fscore 0.60, kappa 0.21, auroc 0.86\n",
      "\n",
      "epoch 26: trainloss 0.1280, valloss 0.0583, precision 0.65, recall 0.60, fscore 0.62, kappa 0.24, auroc 0.86\n",
      "\n",
      "epoch 27: trainloss 0.1276, valloss 0.0659, precision 0.63, recall 0.59, fscore 0.60, kappa 0.20, auroc 0.88\n",
      "\n",
      "epoch 28: trainloss 0.1267, valloss 0.0611, precision 0.64, recall 0.60, fscore 0.61, kappa 0.22, auroc 0.86\n",
      "\n",
      "epoch 29: trainloss 0.1286, valloss 0.0576, precision 0.66, recall 0.60, fscore 0.62, kappa 0.24, auroc 0.89\n",
      "\n",
      "epoch 30: trainloss 0.1250, valloss 0.0587, precision 0.64, recall 0.61, fscore 0.62, kappa 0.24, auroc 0.89\n",
      "\n",
      "epoch 31: trainloss 0.1251, valloss 0.0585, precision 0.65, recall 0.61, fscore 0.62, kappa 0.24, auroc 0.88\n",
      "\n",
      "epoch 32: trainloss 0.1233, valloss 0.0577, precision 0.64, recall 0.61, fscore 0.62, kappa 0.24, auroc 0.90\n",
      "\n",
      "epoch 33: trainloss 0.1236, valloss 0.0567, precision 0.65, recall 0.63, fscore 0.64, kappa 0.27, auroc 0.88\n",
      "\n",
      "epoch 34: trainloss 0.1240, valloss 0.0601, precision 0.64, recall 0.60, fscore 0.61, kappa 0.22, auroc 0.88\n",
      "\n",
      "epoch 35: trainloss 0.1236, valloss 0.0608, precision 0.64, recall 0.62, fscore 0.62, kappa 0.25, auroc 0.90\n",
      "\n",
      "epoch 36: trainloss 0.1213, valloss 0.0554, precision 0.65, recall 0.62, fscore 0.63, kappa 0.27, auroc 0.91\n",
      "\n",
      "epoch 37: trainloss 0.1198, valloss 0.0601, precision 0.63, recall 0.63, fscore 0.62, kappa 0.25, auroc 0.90\n",
      "\n",
      "epoch 38: trainloss 0.1193, valloss 0.0548, precision 0.66, recall 0.64, fscore 0.65, kappa 0.29, auroc 0.89\n",
      "\n",
      "epoch 39: trainloss 0.1182, valloss 0.0534, precision 0.66, recall 0.64, fscore 0.64, kappa 0.29, auroc 0.91\n",
      "\n",
      "epoch 40: trainloss 0.1189, valloss 0.0497, precision 0.68, recall 0.63, fscore 0.65, kappa 0.30, auroc 0.93\n",
      "\n",
      "epoch 41: trainloss 0.1178, valloss 0.0528, precision 0.66, recall 0.62, fscore 0.64, kappa 0.28, auroc 0.90\n",
      "\n",
      "epoch 42: trainloss 0.1167, valloss 0.0524, precision 0.66, recall 0.65, fscore 0.65, kappa 0.31, auroc 0.93\n",
      "\n",
      "epoch 43: trainloss 0.1161, valloss 0.0506, precision 0.67, recall 0.62, fscore 0.64, kappa 0.28, auroc 0.93\n",
      "\n",
      "epoch 44: trainloss 0.1171, valloss 0.0534, precision 0.66, recall 0.63, fscore 0.64, kappa 0.28, auroc 0.90\n",
      "\n",
      "epoch 45: trainloss 0.1163, valloss 0.0503, precision 0.67, recall 0.64, fscore 0.65, kappa 0.30, auroc 0.94\n",
      "\n",
      "epoch 46: trainloss 0.1168, valloss 0.0501, precision 0.66, recall 0.67, fscore 0.66, kappa 0.32, auroc 0.94\n",
      "\n",
      "epoch 47: trainloss 0.1165, valloss 0.0493, precision 0.66, recall 0.64, fscore 0.65, kappa 0.29, auroc 0.94\n",
      "\n",
      "epoch 48: trainloss 0.1144, valloss 0.0511, precision 0.68, recall 0.65, fscore 0.66, kappa 0.32, auroc 0.91\n",
      "\n",
      "epoch 49: trainloss 0.1151, valloss 0.0495, precision 0.66, recall 0.67, fscore 0.66, kappa 0.33, auroc 0.94\n",
      "\n",
      "epoch 50: trainloss 0.1141, valloss 0.0519, precision 0.67, recall 0.66, fscore 0.67, kappa 0.33, auroc 0.92\n",
      "\n",
      "[[0.2015 0.1653 0.1638 0.1614 0.1611 0.1572 0.1565 0.1555 0.1557 0.1539\n",
      "  0.1529 0.1546 0.1534 0.1523 0.1538 0.1512 0.1454 0.1429 0.1376 0.1368\n",
      "  0.1352 0.1345 0.1315 0.131  0.1295 0.128  0.1276 0.1267 0.1286 0.125\n",
      "  0.1251 0.1233 0.1236 0.124  0.1236 0.1213 0.1198 0.1193 0.1182 0.1189\n",
      "  0.1178 0.1167 0.1161 0.1171 0.1163 0.1168 0.1165 0.1144 0.1151 0.1141]\n",
      " [0.0706 0.0699 0.0731 0.0752 0.0713 0.0699 0.0714 0.0701 0.0683 0.0685\n",
      "  0.0711 0.0732 0.0717 0.0696 0.0782 0.0731 0.0645 0.0624 0.0633 0.0616\n",
      "  0.062  0.0607 0.0564 0.062  0.0597 0.0583 0.0659 0.0611 0.0576 0.0587\n",
      "  0.0585 0.0577 0.0567 0.0601 0.0608 0.0554 0.0601 0.0548 0.0534 0.0497\n",
      "  0.0528 0.0524 0.0506 0.0534 0.0503 0.0501 0.0493 0.0511 0.0495 0.0519]\n",
      " [0.59   0.6    0.59   0.55   0.6    0.6    0.58   0.61   0.61   0.58\n",
      "  0.6    0.59   0.56   0.57   0.55   0.55   0.6    0.63   0.58   0.63\n",
      "  0.63   0.63   0.64   0.62   0.62   0.65   0.63   0.64   0.66   0.64\n",
      "  0.65   0.64   0.65   0.64   0.64   0.65   0.63   0.66   0.66   0.68\n",
      "  0.66   0.66   0.67   0.66   0.67   0.66   0.66   0.68   0.66   0.67  ]\n",
      " [0.51   0.52   0.52   0.53   0.51   0.52   0.53   0.51   0.51   0.51\n",
      "  0.51   0.51   0.52   0.52   0.52   0.54   0.54   0.55   0.56   0.57\n",
      "  0.58   0.59   0.61   0.6    0.6    0.6    0.59   0.6    0.6    0.61\n",
      "  0.61   0.61   0.63   0.6    0.62   0.62   0.63   0.64   0.64   0.63\n",
      "  0.62   0.65   0.62   0.63   0.64   0.67   0.64   0.65   0.67   0.66  ]\n",
      " [0.51   0.53   0.52   0.53   0.52   0.52   0.53   0.52   0.52   0.51\n",
      "  0.52   0.51   0.52   0.53   0.52   0.54   0.55   0.57   0.57   0.59\n",
      "  0.59   0.6    0.62   0.61   0.6    0.62   0.6    0.61   0.62   0.62\n",
      "  0.62   0.62   0.64   0.61   0.62   0.63   0.62   0.65   0.64   0.65\n",
      "  0.64   0.65   0.64   0.64   0.65   0.66   0.65   0.66   0.66   0.67  ]\n",
      " [0.03   0.07   0.05   0.07   0.04   0.05   0.06   0.05   0.04   0.02\n",
      "  0.04   0.03   0.05   0.06   0.05   0.08   0.1    0.13   0.14   0.17\n",
      "  0.18   0.21   0.24   0.21   0.21   0.24   0.2    0.22   0.24   0.24\n",
      "  0.24   0.24   0.27   0.22   0.25   0.27   0.25   0.29   0.29   0.3\n",
      "  0.28   0.31   0.28   0.28   0.3    0.32   0.29   0.32   0.33   0.33  ]\n",
      " [0.73   0.77   0.73   0.72   0.72   0.73   0.8    0.74   0.78   0.74\n",
      "  0.79   0.75   0.81   0.81   0.77   0.82   0.83   0.83   0.85   0.84\n",
      "  0.85   0.86   0.89   0.85   0.86   0.86   0.88   0.86   0.89   0.89\n",
      "  0.88   0.9    0.88   0.88   0.9    0.91   0.9    0.89   0.91   0.93\n",
      "  0.9    0.93   0.93   0.9    0.94   0.94   0.94   0.91   0.94   0.92  ]]\n",
      "epoch 1: trainloss 0.2685, valloss 0.2661, precision 0.91, recall 0.68, fscore 0.73, kappa 0.47, auroc 0.92\n",
      "\n",
      "epoch 2: trainloss 0.1243, valloss 0.2286, precision 0.90, recall 0.77, fscore 0.82, kappa 0.64, auroc 0.93\n",
      "\n",
      "epoch 3: trainloss 0.1091, valloss 0.2161, precision 0.86, recall 0.85, fscore 0.85, kappa 0.71, auroc 0.94\n",
      "\n",
      "epoch 4: trainloss 0.1060, valloss 0.2271, precision 0.91, recall 0.71, fscore 0.77, kappa 0.54, auroc 0.94\n",
      "\n",
      "epoch 5: trainloss 0.1081, valloss 0.2164, precision 0.89, recall 0.80, fscore 0.84, kappa 0.68, auroc 0.94\n",
      "\n",
      "epoch 6: trainloss 0.1042, valloss 0.2238, precision 0.89, recall 0.81, fscore 0.84, kappa 0.69, auroc 0.94\n",
      "\n",
      "epoch 7: trainloss 0.1047, valloss 0.2113, precision 0.87, recall 0.83, fscore 0.85, kappa 0.70, auroc 0.94\n",
      "\n",
      "epoch 8: trainloss 0.1053, valloss 0.2119, precision 0.88, recall 0.83, fscore 0.85, kappa 0.71, auroc 0.93\n",
      "\n",
      "epoch 9: trainloss 0.1027, valloss 0.2183, precision 0.77, recall 0.88, fscore 0.81, kappa 0.63, auroc 0.94\n",
      "\n",
      "epoch 10: trainloss 0.1037, valloss 0.2411, precision 0.73, recall 0.87, fscore 0.77, kappa 0.55, auroc 0.93\n",
      "\n",
      "epoch 11: trainloss 0.1016, valloss 0.2149, precision 0.86, recall 0.84, fscore 0.85, kappa 0.70, auroc 0.93\n",
      "\n",
      "epoch 12: trainloss 0.1004, valloss 0.2181, precision 0.79, recall 0.88, fscore 0.83, kappa 0.66, auroc 0.93\n",
      "\n",
      "epoch 13: trainloss 0.0992, valloss 0.3934, precision 0.78, recall 0.54, fscore 0.54, kappa 0.12, auroc 0.78\n",
      "\n",
      "epoch 14: trainloss 0.0990, valloss 0.4336, precision 0.80, recall 0.54, fscore 0.55, kappa 0.14, auroc 0.78\n",
      "\n",
      "epoch 15: trainloss 0.0949, valloss 0.4874, precision 0.84, recall 0.54, fscore 0.54, kappa 0.12, auroc 0.69\n",
      "\n",
      "epoch 16: trainloss 0.0935, valloss 0.2297, precision 0.85, recall 0.75, fscore 0.79, kappa 0.58, auroc 0.92\n",
      "\n",
      "epoch 17: trainloss 0.0916, valloss 0.2474, precision 0.80, recall 0.74, fscore 0.76, kappa 0.53, auroc 0.90\n",
      "\n",
      "epoch 18: trainloss 0.0908, valloss 0.4970, precision 0.73, recall 0.58, fscore 0.60, kappa 0.22, auroc 0.61\n",
      "\n",
      "epoch 19: trainloss 0.0861, valloss 0.2659, precision 0.79, recall 0.70, fscore 0.74, kappa 0.48, auroc 0.89\n",
      "\n",
      "epoch 20: trainloss 0.0848, valloss 0.3848, precision 0.73, recall 0.62, fscore 0.65, kappa 0.30, auroc 0.79\n",
      "\n",
      "epoch 21: trainloss 0.0841, valloss 0.2981, precision 0.66, recall 0.73, fscore 0.68, kappa 0.37, auroc 0.84\n",
      "\n",
      "epoch 22: trainloss 0.0812, valloss 0.2818, precision 0.72, recall 0.69, fscore 0.70, kappa 0.40, auroc 0.86\n",
      "\n",
      "epoch 23: trainloss 0.0803, valloss 0.2647, precision 0.69, recall 0.70, fscore 0.69, kappa 0.39, auroc 0.88\n",
      "\n",
      "epoch 24: trainloss 0.0783, valloss 0.2984, precision 0.76, recall 0.67, fscore 0.70, kappa 0.40, auroc 0.85\n",
      "\n",
      "epoch 25: trainloss 0.0769, valloss 0.2411, precision 0.75, recall 0.79, fscore 0.77, kappa 0.54, auroc 0.90\n",
      "\n",
      "epoch 26: trainloss 0.0773, valloss 0.3056, precision 0.65, recall 0.72, fscore 0.67, kappa 0.35, auroc 0.85\n",
      "\n",
      "epoch 27: trainloss 0.0749, valloss 0.3504, precision 0.74, recall 0.66, fscore 0.69, kappa 0.38, auroc 0.80\n",
      "\n",
      "epoch 28: trainloss 0.0767, valloss 0.2133, precision 0.76, recall 0.87, fscore 0.80, kappa 0.60, auroc 0.94\n",
      "\n",
      "epoch 29: trainloss 0.0748, valloss 0.3047, precision 0.78, recall 0.64, fscore 0.68, kappa 0.36, auroc 0.85\n",
      "\n",
      "epoch 30: trainloss 0.0759, valloss 0.2697, precision 0.69, recall 0.84, fscore 0.71, kappa 0.45, auroc 0.92\n",
      "\n",
      "epoch 31: trainloss 0.0747, valloss 0.2469, precision 0.71, recall 0.84, fscore 0.74, kappa 0.49, auroc 0.92\n",
      "\n",
      "epoch 32: trainloss 0.0740, valloss 0.2552, precision 0.70, recall 0.82, fscore 0.73, kappa 0.48, auroc 0.90\n",
      "\n",
      "epoch 33: trainloss 0.0728, valloss 0.2061, precision 0.75, recall 0.85, fscore 0.79, kappa 0.58, auroc 0.93\n",
      "\n",
      "epoch 34: trainloss 0.0722, valloss 0.2635, precision 0.67, recall 0.80, fscore 0.70, kappa 0.41, auroc 0.90\n",
      "\n",
      "epoch 35: trainloss 0.0711, valloss 0.3550, precision 0.61, recall 0.69, fscore 0.63, kappa 0.27, auroc 0.80\n",
      "\n",
      "epoch 36: trainloss 0.0712, valloss 0.2836, precision 0.69, recall 0.75, fscore 0.71, kappa 0.42, auroc 0.86\n",
      "\n",
      "epoch 37: trainloss 0.0707, valloss 0.4691, precision 0.62, recall 0.77, fscore 0.61, kappa 0.28, auroc 0.82\n",
      "\n",
      "epoch 38: trainloss 0.0724, valloss 0.2488, precision 0.70, recall 0.81, fscore 0.73, kappa 0.48, auroc 0.90\n",
      "\n",
      "epoch 39: trainloss 0.0694, valloss 0.2746, precision 0.67, recall 0.80, fscore 0.69, kappa 0.40, auroc 0.90\n",
      "\n",
      "epoch 40: trainloss 0.0701, valloss 0.3028, precision 0.66, recall 0.78, fscore 0.68, kappa 0.39, auroc 0.88\n",
      "\n",
      "epoch 41: trainloss 0.0688, valloss 0.2890, precision 0.67, recall 0.79, fscore 0.69, kappa 0.40, auroc 0.89\n",
      "\n",
      "epoch 42: trainloss 0.0688, valloss 0.3205, precision 0.64, recall 0.75, fscore 0.65, kappa 0.33, auroc 0.85\n",
      "\n",
      "epoch 43: trainloss 0.0694, valloss 0.2432, precision 0.70, recall 0.84, fscore 0.73, kappa 0.47, auroc 0.93\n",
      "\n",
      "epoch 44: trainloss 0.0683, valloss 0.6948, precision 0.60, recall 0.69, fscore 0.59, kappa 0.23, auroc 0.73\n",
      "\n",
      "epoch 45: trainloss 0.0679, valloss 0.2918, precision 0.66, recall 0.76, fscore 0.68, kappa 0.38, auroc 0.87\n",
      "\n",
      "epoch 46: trainloss 0.0673, valloss 0.4550, precision 0.61, recall 0.72, fscore 0.61, kappa 0.27, auroc 0.79\n",
      "\n",
      "epoch 47: trainloss 0.0682, valloss 0.3107, precision 0.66, recall 0.77, fscore 0.68, kappa 0.38, auroc 0.86\n",
      "\n",
      "epoch 48: trainloss 0.0665, valloss 0.3520, precision 0.64, recall 0.76, fscore 0.66, kappa 0.34, auroc 0.84\n",
      "\n",
      "epoch 49: trainloss 0.0678, valloss 0.3388, precision 0.64, recall 0.77, fscore 0.65, kappa 0.34, auroc 0.86\n",
      "\n",
      "epoch 50: trainloss 0.0670, valloss 0.4059, precision 0.62, recall 0.74, fscore 0.63, kappa 0.29, auroc 0.81\n",
      "\n",
      "[[0.2685 0.1243 0.1091 0.106  0.1081 0.1042 0.1047 0.1053 0.1027 0.1037\n",
      "  0.1016 0.1004 0.0992 0.099  0.0949 0.0935 0.0916 0.0908 0.0861 0.0848\n",
      "  0.0841 0.0812 0.0803 0.0783 0.0769 0.0773 0.0749 0.0767 0.0748 0.0759\n",
      "  0.0747 0.074  0.0728 0.0722 0.0711 0.0712 0.0707 0.0724 0.0694 0.0701\n",
      "  0.0688 0.0688 0.0694 0.0683 0.0679 0.0673 0.0682 0.0665 0.0678 0.067 ]\n",
      " [0.2661 0.2286 0.2161 0.2271 0.2164 0.2238 0.2113 0.2119 0.2183 0.2411\n",
      "  0.2149 0.2181 0.3934 0.4336 0.4874 0.2297 0.2474 0.497  0.2659 0.3848\n",
      "  0.2981 0.2818 0.2647 0.2984 0.2411 0.3056 0.3504 0.2133 0.3047 0.2697\n",
      "  0.2469 0.2552 0.2061 0.2635 0.355  0.2836 0.4691 0.2488 0.2746 0.3028\n",
      "  0.289  0.3205 0.2432 0.6948 0.2918 0.455  0.3107 0.352  0.3388 0.4059]\n",
      " [0.91   0.9    0.86   0.91   0.89   0.89   0.87   0.88   0.77   0.73\n",
      "  0.86   0.79   0.78   0.8    0.84   0.85   0.8    0.73   0.79   0.73\n",
      "  0.66   0.72   0.69   0.76   0.75   0.65   0.74   0.76   0.78   0.69\n",
      "  0.71   0.7    0.75   0.67   0.61   0.69   0.62   0.7    0.67   0.66\n",
      "  0.67   0.64   0.7    0.6    0.66   0.61   0.66   0.64   0.64   0.62  ]\n",
      " [0.68   0.77   0.85   0.71   0.8    0.81   0.83   0.83   0.88   0.87\n",
      "  0.84   0.88   0.54   0.54   0.54   0.75   0.74   0.58   0.7    0.62\n",
      "  0.73   0.69   0.7    0.67   0.79   0.72   0.66   0.87   0.64   0.84\n",
      "  0.84   0.82   0.85   0.8    0.69   0.75   0.77   0.81   0.8    0.78\n",
      "  0.79   0.75   0.84   0.69   0.76   0.72   0.77   0.76   0.77   0.74  ]\n",
      " [0.73   0.82   0.85   0.77   0.84   0.84   0.85   0.85   0.81   0.77\n",
      "  0.85   0.83   0.54   0.55   0.54   0.79   0.76   0.6    0.74   0.65\n",
      "  0.68   0.7    0.69   0.7    0.77   0.67   0.69   0.8    0.68   0.71\n",
      "  0.74   0.73   0.79   0.7    0.63   0.71   0.61   0.73   0.69   0.68\n",
      "  0.69   0.65   0.73   0.59   0.68   0.61   0.68   0.66   0.65   0.63  ]\n",
      " [0.47   0.64   0.71   0.54   0.68   0.69   0.7    0.71   0.63   0.55\n",
      "  0.7    0.66   0.12   0.14   0.12   0.58   0.53   0.22   0.48   0.3\n",
      "  0.37   0.4    0.39   0.4    0.54   0.35   0.38   0.6    0.36   0.45\n",
      "  0.49   0.48   0.58   0.41   0.27   0.42   0.28   0.48   0.4    0.39\n",
      "  0.4    0.33   0.47   0.23   0.38   0.27   0.38   0.34   0.34   0.29  ]\n",
      " [0.92   0.93   0.94   0.94   0.94   0.94   0.94   0.93   0.94   0.93\n",
      "  0.93   0.93   0.78   0.78   0.69   0.92   0.9    0.61   0.89   0.79\n",
      "  0.84   0.86   0.88   0.85   0.9    0.85   0.8    0.94   0.85   0.92\n",
      "  0.92   0.9    0.93   0.9    0.8    0.86   0.82   0.9    0.9    0.88\n",
      "  0.89   0.85   0.93   0.73   0.87   0.79   0.86   0.84   0.86   0.81  ]]\n",
      "epoch 1: trainloss 0.1944, valloss 0.3491, precision 0.79, recall 0.63, fscore 0.67, kappa 0.35, auroc 0.84\n",
      "\n",
      "epoch 2: trainloss 0.1447, valloss 0.3261, precision 0.80, recall 0.61, fscore 0.64, kappa 0.30, auroc 0.85\n",
      "\n",
      "epoch 3: trainloss 0.1400, valloss 0.5086, precision 0.82, recall 0.54, fscore 0.54, kappa 0.12, auroc 0.79\n",
      "\n",
      "epoch 4: trainloss 0.1386, valloss 0.3510, precision 0.83, recall 0.57, fscore 0.59, kappa 0.21, auroc 0.86\n",
      "\n",
      "epoch 5: trainloss 0.1373, valloss 0.3169, precision 0.78, recall 0.64, fscore 0.68, kappa 0.37, auroc 0.84\n",
      "\n",
      "epoch 6: trainloss 0.1360, valloss 0.4069, precision 0.82, recall 0.57, fscore 0.58, kappa 0.20, auroc 0.84\n",
      "\n",
      "epoch 7: trainloss 0.1335, valloss 0.3792, precision 0.80, recall 0.58, fscore 0.61, kappa 0.24, auroc 0.85\n",
      "\n",
      "epoch 8: trainloss 0.1301, valloss 0.3560, precision 0.81, recall 0.58, fscore 0.60, kappa 0.22, auroc 0.85\n",
      "\n",
      "epoch 9: trainloss 0.1284, valloss 0.4195, precision 0.82, recall 0.60, fscore 0.63, kappa 0.28, auroc 0.84\n",
      "\n",
      "epoch 10: trainloss 0.1273, valloss 0.4029, precision 0.83, recall 0.57, fscore 0.59, kappa 0.22, auroc 0.84\n",
      "\n",
      "epoch 11: trainloss 0.1260, valloss 0.3896, precision 0.81, recall 0.59, fscore 0.62, kappa 0.26, auroc 0.85\n",
      "\n",
      "epoch 12: trainloss 0.1249, valloss 0.3834, precision 0.80, recall 0.59, fscore 0.62, kappa 0.27, auroc 0.85\n",
      "\n",
      "epoch 13: trainloss 0.1240, valloss 0.4895, precision 0.81, recall 0.56, fscore 0.58, kappa 0.19, auroc 0.80\n",
      "\n",
      "epoch 14: trainloss 0.1236, valloss 0.3344, precision 0.78, recall 0.61, fscore 0.65, kappa 0.31, auroc 0.84\n",
      "\n",
      "epoch 15: trainloss 0.1216, valloss 0.3449, precision 0.79, recall 0.64, fscore 0.68, kappa 0.36, auroc 0.87\n",
      "\n",
      "epoch 16: trainloss 0.1197, valloss 0.4057, precision 0.79, recall 0.56, fscore 0.58, kappa 0.19, auroc 0.83\n",
      "\n",
      "epoch 17: trainloss 0.1181, valloss 0.3200, precision 0.79, recall 0.64, fscore 0.68, kappa 0.37, auroc 0.86\n",
      "\n",
      "epoch 18: trainloss 0.1169, valloss 0.3410, precision 0.81, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.89\n",
      "\n",
      "epoch 19: trainloss 0.1168, valloss 0.3462, precision 0.81, recall 0.64, fscore 0.68, kappa 0.38, auroc 0.88\n",
      "\n",
      "epoch 20: trainloss 0.1167, valloss 0.3840, precision 0.81, recall 0.62, fscore 0.65, kappa 0.32, auroc 0.86\n",
      "\n",
      "epoch 21: trainloss 0.1161, valloss 0.2892, precision 0.80, recall 0.69, fscore 0.73, kappa 0.46, auroc 0.89\n",
      "\n",
      "epoch 22: trainloss 0.1145, valloss 0.3571, precision 0.81, recall 0.63, fscore 0.66, kappa 0.34, auroc 0.85\n",
      "\n",
      "epoch 23: trainloss 0.1139, valloss 0.2907, precision 0.78, recall 0.69, fscore 0.72, kappa 0.45, auroc 0.86\n",
      "\n",
      "epoch 24: trainloss 0.1122, valloss 0.3428, precision 0.80, recall 0.64, fscore 0.68, kappa 0.38, auroc 0.88\n",
      "\n",
      "epoch 25: trainloss 0.1120, valloss 0.2978, precision 0.79, recall 0.68, fscore 0.72, kappa 0.44, auroc 0.86\n",
      "\n",
      "epoch 26: trainloss 0.1121, valloss 0.3740, precision 0.80, recall 0.64, fscore 0.68, kappa 0.37, auroc 0.83\n",
      "\n",
      "epoch 27: trainloss 0.1113, valloss 0.3300, precision 0.81, recall 0.65, fscore 0.69, kappa 0.39, auroc 0.88\n",
      "\n",
      "epoch 28: trainloss 0.1118, valloss 0.4017, precision 0.82, recall 0.61, fscore 0.65, kappa 0.32, auroc 0.86\n",
      "\n",
      "epoch 29: trainloss 0.1111, valloss 0.3761, precision 0.81, recall 0.65, fscore 0.70, kappa 0.40, auroc 0.86\n",
      "\n",
      "epoch 30: trainloss 0.1099, valloss 0.3396, precision 0.81, recall 0.69, fscore 0.73, kappa 0.46, auroc 0.87\n",
      "\n",
      "epoch 31: trainloss 0.1092, valloss 0.3002, precision 0.83, recall 0.71, fscore 0.75, kappa 0.50, auroc 0.90\n",
      "\n",
      "epoch 32: trainloss 0.1090, valloss 0.3228, precision 0.81, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.87\n",
      "\n",
      "epoch 33: trainloss 0.1086, valloss 0.2878, precision 0.84, recall 0.69, fscore 0.74, kappa 0.48, auroc 0.89\n",
      "\n",
      "epoch 34: trainloss 0.1090, valloss 0.3053, precision 0.82, recall 0.71, fscore 0.75, kappa 0.50, auroc 0.88\n",
      "\n",
      "epoch 35: trainloss 0.1087, valloss 0.3497, precision 0.84, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.87\n",
      "\n",
      "epoch 36: trainloss 0.1078, valloss 0.3647, precision 0.84, recall 0.65, fscore 0.69, kappa 0.40, auroc 0.88\n",
      "\n",
      "epoch 37: trainloss 0.1076, valloss 0.3625, precision 0.82, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.87\n",
      "\n",
      "epoch 38: trainloss 0.1066, valloss 0.2727, precision 0.82, recall 0.72, fscore 0.75, kappa 0.51, auroc 0.90\n",
      "\n",
      "epoch 39: trainloss 0.1068, valloss 0.3224, precision 0.81, recall 0.66, fscore 0.70, kappa 0.41, auroc 0.88\n",
      "\n",
      "epoch 40: trainloss 0.1066, valloss 0.3565, precision 0.84, recall 0.62, fscore 0.66, kappa 0.34, auroc 0.88\n",
      "\n",
      "epoch 41: trainloss 0.1069, valloss 0.3275, precision 0.83, recall 0.63, fscore 0.67, kappa 0.36, auroc 0.89\n",
      "\n",
      "epoch 42: trainloss 0.1069, valloss 0.3667, precision 0.82, recall 0.65, fscore 0.70, kappa 0.40, auroc 0.88\n",
      "\n",
      "epoch 43: trainloss 0.1062, valloss 0.3309, precision 0.81, recall 0.65, fscore 0.69, kappa 0.38, auroc 0.88\n",
      "\n",
      "epoch 44: trainloss 0.1058, valloss 0.3057, precision 0.82, recall 0.66, fscore 0.70, kappa 0.42, auroc 0.90\n",
      "\n",
      "epoch 45: trainloss 0.1039, valloss 0.3493, precision 0.85, recall 0.65, fscore 0.69, kappa 0.40, auroc 0.91\n",
      "\n",
      "epoch 46: trainloss 0.1063, valloss 0.3675, precision 0.83, recall 0.65, fscore 0.70, kappa 0.40, auroc 0.88\n",
      "\n",
      "epoch 47: trainloss 0.1053, valloss 0.3564, precision 0.82, recall 0.62, fscore 0.65, kappa 0.33, auroc 0.90\n",
      "\n",
      "epoch 48: trainloss 0.1040, valloss 0.3681, precision 0.83, recall 0.67, fscore 0.72, kappa 0.44, auroc 0.88\n",
      "\n",
      "epoch 49: trainloss 0.1048, valloss 0.3601, precision 0.83, recall 0.65, fscore 0.69, kappa 0.39, auroc 0.89\n",
      "\n",
      "epoch 50: trainloss 0.1044, valloss 0.3292, precision 0.84, recall 0.65, fscore 0.70, kappa 0.41, auroc 0.90\n",
      "\n",
      "[[0.1944 0.1447 0.14   0.1386 0.1373 0.136  0.1335 0.1301 0.1284 0.1273\n",
      "  0.126  0.1249 0.124  0.1236 0.1216 0.1197 0.1181 0.1169 0.1168 0.1167\n",
      "  0.1161 0.1145 0.1139 0.1122 0.112  0.1121 0.1113 0.1118 0.1111 0.1099\n",
      "  0.1092 0.109  0.1086 0.109  0.1087 0.1078 0.1076 0.1066 0.1068 0.1066\n",
      "  0.1069 0.1069 0.1062 0.1058 0.1039 0.1063 0.1053 0.104  0.1048 0.1044]\n",
      " [0.3491 0.3261 0.5086 0.351  0.3169 0.4069 0.3792 0.356  0.4195 0.4029\n",
      "  0.3896 0.3834 0.4895 0.3344 0.3449 0.4057 0.32   0.341  0.3462 0.384\n",
      "  0.2892 0.3571 0.2907 0.3428 0.2978 0.374  0.33   0.4017 0.3761 0.3396\n",
      "  0.3002 0.3228 0.2878 0.3053 0.3497 0.3647 0.3625 0.2727 0.3224 0.3565\n",
      "  0.3275 0.3667 0.3309 0.3057 0.3493 0.3675 0.3564 0.3681 0.3601 0.3292]\n",
      " [0.79   0.8    0.82   0.83   0.78   0.82   0.8    0.81   0.82   0.83\n",
      "  0.81   0.8    0.81   0.78   0.79   0.79   0.79   0.81   0.81   0.81\n",
      "  0.8    0.81   0.78   0.8    0.79   0.8    0.81   0.82   0.81   0.81\n",
      "  0.83   0.81   0.84   0.82   0.84   0.84   0.82   0.82   0.81   0.84\n",
      "  0.83   0.82   0.81   0.82   0.85   0.83   0.82   0.83   0.83   0.84  ]\n",
      " [0.63   0.61   0.54   0.57   0.64   0.57   0.58   0.58   0.6    0.57\n",
      "  0.59   0.59   0.56   0.61   0.64   0.56   0.64   0.67   0.64   0.62\n",
      "  0.69   0.63   0.69   0.64   0.68   0.64   0.65   0.61   0.65   0.69\n",
      "  0.71   0.67   0.69   0.71   0.67   0.65   0.67   0.72   0.66   0.62\n",
      "  0.63   0.65   0.65   0.66   0.65   0.65   0.62   0.67   0.65   0.65  ]\n",
      " [0.67   0.64   0.54   0.59   0.68   0.58   0.61   0.6    0.63   0.59\n",
      "  0.62   0.62   0.58   0.65   0.68   0.58   0.68   0.71   0.68   0.65\n",
      "  0.73   0.66   0.72   0.68   0.72   0.68   0.69   0.65   0.7    0.73\n",
      "  0.75   0.71   0.74   0.75   0.71   0.69   0.71   0.75   0.7    0.66\n",
      "  0.67   0.7    0.69   0.7    0.69   0.7    0.65   0.72   0.69   0.7   ]\n",
      " [0.35   0.3    0.12   0.21   0.37   0.2    0.24   0.22   0.28   0.22\n",
      "  0.26   0.27   0.19   0.31   0.36   0.19   0.37   0.43   0.38   0.32\n",
      "  0.46   0.34   0.45   0.38   0.44   0.37   0.39   0.32   0.4    0.46\n",
      "  0.5    0.43   0.48   0.5    0.43   0.4    0.43   0.51   0.41   0.34\n",
      "  0.36   0.4    0.38   0.42   0.4    0.4    0.33   0.44   0.39   0.41  ]\n",
      " [0.84   0.85   0.79   0.86   0.84   0.84   0.85   0.85   0.84   0.84\n",
      "  0.85   0.85   0.8    0.84   0.87   0.83   0.86   0.89   0.88   0.86\n",
      "  0.89   0.85   0.86   0.88   0.86   0.83   0.88   0.86   0.86   0.87\n",
      "  0.9    0.87   0.89   0.88   0.87   0.88   0.87   0.9    0.88   0.88\n",
      "  0.89   0.88   0.88   0.9    0.91   0.88   0.9    0.88   0.89   0.9   ]]\n",
      "epoch 1: trainloss 0.1951, valloss 0.4714, precision 0.87, recall 0.55, fscore 0.53, kappa 0.15, auroc 0.89\n",
      "\n",
      "epoch 2: trainloss 0.0983, valloss 0.4839, precision 0.38, recall 0.50, fscore 0.43, kappa 0.00, auroc 0.91\n",
      "\n",
      "epoch 3: trainloss 0.0938, valloss 0.6677, precision 0.84, recall 0.50, fscore 0.43, kappa 0.00, auroc 0.76\n",
      "\n",
      "epoch 4: trainloss 0.0917, valloss 0.3661, precision 0.79, recall 0.83, fscore 0.81, kappa 0.61, auroc 0.90\n",
      "\n",
      "epoch 5: trainloss 0.0892, valloss 0.3903, precision 0.73, recall 0.80, fscore 0.74, kappa 0.49, auroc 0.89\n",
      "\n",
      "epoch 6: trainloss 0.0872, valloss 0.3652, precision 0.76, recall 0.82, fscore 0.77, kappa 0.56, auroc 0.90\n",
      "\n",
      "epoch 7: trainloss 0.0854, valloss 0.3756, precision 0.82, recall 0.80, fscore 0.81, kappa 0.62, auroc 0.90\n",
      "\n",
      "epoch 8: trainloss 0.0830, valloss 0.4103, precision 0.76, recall 0.80, fscore 0.78, kappa 0.56, auroc 0.87\n",
      "\n",
      "epoch 9: trainloss 0.0817, valloss 0.3938, precision 0.75, recall 0.79, fscore 0.77, kappa 0.54, auroc 0.88\n",
      "\n",
      "epoch 10: trainloss 0.0806, valloss 0.3631, precision 0.79, recall 0.80, fscore 0.79, kappa 0.59, auroc 0.90\n",
      "\n",
      "epoch 11: trainloss 0.0783, valloss 0.4225, precision 0.74, recall 0.78, fscore 0.75, kappa 0.51, auroc 0.86\n",
      "\n",
      "epoch 12: trainloss 0.0787, valloss 0.3971, precision 0.79, recall 0.77, fscore 0.78, kappa 0.56, auroc 0.88\n",
      "\n",
      "epoch 13: trainloss 0.0771, valloss 0.5357, precision 0.70, recall 0.65, fscore 0.67, kappa 0.34, auroc 0.79\n",
      "\n",
      "epoch 14: trainloss 0.0759, valloss 0.3759, precision 0.75, recall 0.80, fscore 0.77, kappa 0.54, auroc 0.88\n",
      "\n",
      "epoch 15: trainloss 0.0737, valloss 0.4016, precision 0.75, recall 0.76, fscore 0.76, kappa 0.51, auroc 0.86\n",
      "\n",
      "epoch 16: trainloss 0.0751, valloss 0.4182, precision 0.71, recall 0.75, fscore 0.73, kappa 0.46, auroc 0.84\n",
      "\n",
      "epoch 17: trainloss 0.0719, valloss 0.3852, precision 0.74, recall 0.79, fscore 0.75, kappa 0.51, auroc 0.87\n",
      "\n",
      "epoch 18: trainloss 0.0706, valloss 0.3733, precision 0.73, recall 0.78, fscore 0.75, kappa 0.50, auroc 0.88\n",
      "\n",
      "epoch 19: trainloss 0.0712, valloss 0.4237, precision 0.71, recall 0.75, fscore 0.72, kappa 0.45, auroc 0.83\n",
      "\n",
      "epoch 20: trainloss 0.0695, valloss 0.3917, precision 0.71, recall 0.78, fscore 0.72, kappa 0.46, auroc 0.87\n",
      "\n",
      "epoch 21: trainloss 0.0667, valloss 0.4818, precision 0.73, recall 0.69, fscore 0.70, kappa 0.40, auroc 0.83\n",
      "\n",
      "epoch 22: trainloss 0.0650, valloss 0.4100, precision 0.71, recall 0.77, fscore 0.72, kappa 0.46, auroc 0.85\n",
      "\n",
      "epoch 23: trainloss 0.0645, valloss 0.3718, precision 0.72, recall 0.79, fscore 0.73, kappa 0.48, auroc 0.87\n",
      "\n",
      "epoch 24: trainloss 0.0625, valloss 0.3821, precision 0.73, recall 0.79, fscore 0.74, kappa 0.49, auroc 0.86\n",
      "\n",
      "epoch 25: trainloss 0.0616, valloss 0.4366, precision 0.70, recall 0.77, fscore 0.69, kappa 0.42, auroc 0.85\n",
      "\n",
      "epoch 26: trainloss 0.0600, valloss 0.4090, precision 0.70, recall 0.76, fscore 0.71, kappa 0.43, auroc 0.86\n",
      "\n",
      "epoch 27: trainloss 0.0596, valloss 0.4690, precision 0.66, recall 0.72, fscore 0.66, kappa 0.35, auroc 0.81\n",
      "\n",
      "epoch 28: trainloss 0.0591, valloss 0.4233, precision 0.71, recall 0.76, fscore 0.72, kappa 0.44, auroc 0.84\n",
      "\n",
      "epoch 29: trainloss 0.0580, valloss 0.4393, precision 0.69, recall 0.77, fscore 0.68, kappa 0.40, auroc 0.85\n",
      "\n",
      "epoch 30: trainloss 0.0570, valloss 0.4200, precision 0.70, recall 0.76, fscore 0.71, kappa 0.44, auroc 0.85\n",
      "\n",
      "epoch 31: trainloss 0.0573, valloss 0.4162, precision 0.70, recall 0.75, fscore 0.71, kappa 0.43, auroc 0.86\n",
      "\n",
      "epoch 32: trainloss 0.0569, valloss 0.4745, precision 0.68, recall 0.75, fscore 0.68, kappa 0.38, auroc 0.84\n",
      "\n",
      "epoch 33: trainloss 0.0560, valloss 0.4200, precision 0.70, recall 0.77, fscore 0.70, kappa 0.43, auroc 0.86\n",
      "\n",
      "epoch 34: trainloss 0.0555, valloss 0.4266, precision 0.70, recall 0.77, fscore 0.70, kappa 0.42, auroc 0.86\n",
      "\n",
      "epoch 35: trainloss 0.0554, valloss 0.4455, precision 0.69, recall 0.73, fscore 0.70, kappa 0.41, auroc 0.84\n",
      "\n",
      "epoch 36: trainloss 0.0556, valloss 0.4377, precision 0.70, recall 0.76, fscore 0.71, kappa 0.44, auroc 0.85\n",
      "\n",
      "epoch 37: trainloss 0.0550, valloss 0.4193, precision 0.70, recall 0.75, fscore 0.71, kappa 0.43, auroc 0.85\n",
      "\n",
      "epoch 38: trainloss 0.0539, valloss 0.3829, precision 0.71, recall 0.77, fscore 0.73, kappa 0.46, auroc 0.87\n",
      "\n",
      "epoch 39: trainloss 0.0553, valloss 0.4490, precision 0.73, recall 0.68, fscore 0.70, kappa 0.40, auroc 0.85\n",
      "\n",
      "epoch 40: trainloss 0.0544, valloss 0.4164, precision 0.71, recall 0.78, fscore 0.71, kappa 0.44, auroc 0.87\n",
      "\n",
      "epoch 41: trainloss 0.0533, valloss 0.4408, precision 0.69, recall 0.75, fscore 0.70, kappa 0.41, auroc 0.84\n",
      "\n",
      "epoch 42: trainloss 0.0531, valloss 0.4905, precision 0.67, recall 0.73, fscore 0.66, kappa 0.36, auroc 0.83\n",
      "\n",
      "epoch 43: trainloss 0.0539, valloss 0.4487, precision 0.68, recall 0.73, fscore 0.69, kappa 0.39, auroc 0.83\n",
      "\n",
      "epoch 44: trainloss 0.0525, valloss 0.4746, precision 0.69, recall 0.75, fscore 0.70, kappa 0.41, auroc 0.84\n",
      "\n",
      "epoch 45: trainloss 0.0523, valloss 0.3918, precision 0.74, recall 0.76, fscore 0.75, kappa 0.49, auroc 0.87\n",
      "\n",
      "epoch 46: trainloss 0.0521, valloss 0.4595, precision 0.69, recall 0.76, fscore 0.69, kappa 0.41, auroc 0.85\n",
      "\n",
      "epoch 47: trainloss 0.0529, valloss 0.3822, precision 0.72, recall 0.80, fscore 0.73, kappa 0.48, auroc 0.89\n",
      "\n",
      "epoch 48: trainloss 0.0526, valloss 0.4698, precision 0.67, recall 0.72, fscore 0.68, kappa 0.37, auroc 0.83\n",
      "\n",
      "epoch 49: trainloss 0.0518, valloss 0.4451, precision 0.70, recall 0.76, fscore 0.71, kappa 0.43, auroc 0.85\n",
      "\n",
      "epoch 50: trainloss 0.0517, valloss 0.4430, precision 0.69, recall 0.74, fscore 0.70, kappa 0.41, auroc 0.84\n",
      "\n",
      "[[0.1951 0.0983 0.0938 0.0917 0.0892 0.0872 0.0854 0.083  0.0817 0.0806\n",
      "  0.0783 0.0787 0.0771 0.0759 0.0737 0.0751 0.0719 0.0706 0.0712 0.0695\n",
      "  0.0667 0.065  0.0645 0.0625 0.0616 0.06   0.0596 0.0591 0.058  0.057\n",
      "  0.0573 0.0569 0.056  0.0555 0.0554 0.0556 0.055  0.0539 0.0553 0.0544\n",
      "  0.0533 0.0531 0.0539 0.0525 0.0523 0.0521 0.0529 0.0526 0.0518 0.0517]\n",
      " [0.4714 0.4839 0.6677 0.3661 0.3903 0.3652 0.3756 0.4103 0.3938 0.3631\n",
      "  0.4225 0.3971 0.5357 0.3759 0.4016 0.4182 0.3852 0.3733 0.4237 0.3917\n",
      "  0.4818 0.41   0.3718 0.3821 0.4366 0.409  0.469  0.4233 0.4393 0.42\n",
      "  0.4162 0.4745 0.42   0.4266 0.4455 0.4377 0.4193 0.3829 0.449  0.4164\n",
      "  0.4408 0.4905 0.4487 0.4746 0.3918 0.4595 0.3822 0.4698 0.4451 0.443 ]\n",
      " [0.87   0.38   0.84   0.79   0.73   0.76   0.82   0.76   0.75   0.79\n",
      "  0.74   0.79   0.7    0.75   0.75   0.71   0.74   0.73   0.71   0.71\n",
      "  0.73   0.71   0.72   0.73   0.7    0.7    0.66   0.71   0.69   0.7\n",
      "  0.7    0.68   0.7    0.7    0.69   0.7    0.7    0.71   0.73   0.71\n",
      "  0.69   0.67   0.68   0.69   0.74   0.69   0.72   0.67   0.7    0.69  ]\n",
      " [0.55   0.5    0.5    0.83   0.8    0.82   0.8    0.8    0.79   0.8\n",
      "  0.78   0.77   0.65   0.8    0.76   0.75   0.79   0.78   0.75   0.78\n",
      "  0.69   0.77   0.79   0.79   0.77   0.76   0.72   0.76   0.77   0.76\n",
      "  0.75   0.75   0.77   0.77   0.73   0.76   0.75   0.77   0.68   0.78\n",
      "  0.75   0.73   0.73   0.75   0.76   0.76   0.8    0.72   0.76   0.74  ]\n",
      " [0.53   0.43   0.43   0.81   0.74   0.77   0.81   0.78   0.77   0.79\n",
      "  0.75   0.78   0.67   0.77   0.76   0.73   0.75   0.75   0.72   0.72\n",
      "  0.7    0.72   0.73   0.74   0.69   0.71   0.66   0.72   0.68   0.71\n",
      "  0.71   0.68   0.7    0.7    0.7    0.71   0.71   0.73   0.7    0.71\n",
      "  0.7    0.66   0.69   0.7    0.75   0.69   0.73   0.68   0.71   0.7   ]\n",
      " [0.15   0.     0.     0.61   0.49   0.56   0.62   0.56   0.54   0.59\n",
      "  0.51   0.56   0.34   0.54   0.51   0.46   0.51   0.5    0.45   0.46\n",
      "  0.4    0.46   0.48   0.49   0.42   0.43   0.35   0.44   0.4    0.44\n",
      "  0.43   0.38   0.43   0.42   0.41   0.44   0.43   0.46   0.4    0.44\n",
      "  0.41   0.36   0.39   0.41   0.49   0.41   0.48   0.37   0.43   0.41  ]\n",
      " [0.89   0.91   0.76   0.9    0.89   0.9    0.9    0.87   0.88   0.9\n",
      "  0.86   0.88   0.79   0.88   0.86   0.84   0.87   0.88   0.83   0.87\n",
      "  0.83   0.85   0.87   0.86   0.85   0.86   0.81   0.84   0.85   0.85\n",
      "  0.86   0.84   0.86   0.86   0.84   0.85   0.85   0.87   0.85   0.87\n",
      "  0.84   0.83   0.83   0.84   0.87   0.85   0.89   0.83   0.85   0.84  ]]\n",
      "None\n",
      "epoch 1: trainloss 0.2275, valloss 0.0717, precision 0.54, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.76\n",
      "\n",
      "epoch 2: trainloss 0.1600, valloss 0.1006, precision 0.54, recall 0.58, fscore 0.55, kappa 0.10, auroc 0.72\n",
      "\n",
      "epoch 3: trainloss 0.1511, valloss 0.0741, precision 0.54, recall 0.57, fscore 0.55, kappa 0.11, auroc 0.77\n",
      "\n",
      "epoch 4: trainloss 0.1447, valloss 0.0683, precision 0.55, recall 0.55, fscore 0.55, kappa 0.10, auroc 0.73\n",
      "\n",
      "epoch 5: trainloss 0.1403, valloss 0.0683, precision 0.55, recall 0.56, fscore 0.55, kappa 0.11, auroc 0.79\n",
      "\n",
      "epoch 6: trainloss 0.1350, valloss 0.0657, precision 0.56, recall 0.55, fscore 0.55, kappa 0.11, auroc 0.81\n",
      "\n",
      "epoch 7: trainloss 0.1318, valloss 0.0597, precision 0.55, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.82\n",
      "\n",
      "epoch 8: trainloss 0.1290, valloss 0.0604, precision 0.55, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.83\n",
      "\n",
      "epoch 9: trainloss 0.1272, valloss 0.0642, precision 0.56, recall 0.54, fscore 0.55, kappa 0.10, auroc 0.81\n",
      "\n",
      "epoch 10: trainloss 0.1243, valloss 0.0595, precision 0.56, recall 0.56, fscore 0.56, kappa 0.11, auroc 0.86\n",
      "\n",
      "epoch 11: trainloss 0.1219, valloss 0.0625, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.81\n",
      "\n",
      "epoch 12: trainloss 0.1207, valloss 0.0599, precision 0.55, recall 0.52, fscore 0.53, kappa 0.06, auroc 0.82\n",
      "\n",
      "epoch 13: trainloss 0.1196, valloss 0.0608, precision 0.56, recall 0.56, fscore 0.56, kappa 0.11, auroc 0.85\n",
      "\n",
      "epoch 14: trainloss 0.1187, valloss 0.0576, precision 0.56, recall 0.52, fscore 0.52, kappa 0.05, auroc 0.84\n",
      "\n",
      "epoch 15: trainloss 0.1172, valloss 0.0578, precision 0.56, recall 0.52, fscore 0.53, kappa 0.06, auroc 0.85\n",
      "\n",
      "epoch 16: trainloss 0.1156, valloss 0.0564, precision 0.56, recall 0.52, fscore 0.53, kappa 0.07, auroc 0.86\n",
      "\n",
      "epoch 17: trainloss 0.1146, valloss 0.0559, precision 0.55, recall 0.51, fscore 0.51, kappa 0.03, auroc 0.85\n",
      "\n",
      "epoch 18: trainloss 0.1132, valloss 0.0563, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.86\n",
      "\n",
      "epoch 19: trainloss 0.1142, valloss 0.0566, precision 0.55, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.88\n",
      "\n",
      "epoch 20: trainloss 0.1141, valloss 0.0537, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.88\n",
      "\n",
      "epoch 21: trainloss 0.1128, valloss 0.0546, precision 0.57, recall 0.58, fscore 0.58, kappa 0.15, auroc 0.89\n",
      "\n",
      "epoch 22: trainloss 0.1119, valloss 0.0550, precision 0.57, recall 0.55, fscore 0.56, kappa 0.12, auroc 0.89\n",
      "\n",
      "epoch 23: trainloss 0.1107, valloss 0.0542, precision 0.57, recall 0.57, fscore 0.57, kappa 0.14, auroc 0.90\n",
      "\n",
      "epoch 24: trainloss 0.1097, valloss 0.0546, precision 0.57, recall 0.56, fscore 0.56, kappa 0.12, auroc 0.89\n",
      "\n",
      "epoch 25: trainloss 0.1101, valloss 0.0533, precision 0.57, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.89\n",
      "\n",
      "epoch 26: trainloss 0.1090, valloss 0.0515, precision 0.58, recall 0.55, fscore 0.56, kappa 0.11, auroc 0.90\n",
      "\n",
      "epoch 27: trainloss 0.1090, valloss 0.0530, precision 0.60, recall 0.55, fscore 0.56, kappa 0.13, auroc 0.88\n",
      "\n",
      "epoch 28: trainloss 0.1083, valloss 0.0545, precision 0.57, recall 0.58, fscore 0.58, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 29: trainloss 0.1091, valloss 0.0533, precision 0.58, recall 0.58, fscore 0.58, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 30: trainloss 0.1075, valloss 0.0527, precision 0.58, recall 0.58, fscore 0.58, kappa 0.16, auroc 0.90\n",
      "\n",
      "epoch 31: trainloss 0.1081, valloss 0.0497, precision 0.61, recall 0.54, fscore 0.56, kappa 0.12, auroc 0.90\n",
      "\n",
      "epoch 32: trainloss 0.1072, valloss 0.0503, precision 0.62, recall 0.55, fscore 0.57, kappa 0.13, auroc 0.89\n",
      "\n",
      "epoch 33: trainloss 0.1070, valloss 0.0519, precision 0.59, recall 0.58, fscore 0.59, kappa 0.17, auroc 0.90\n",
      "\n",
      "epoch 34: trainloss 0.1058, valloss 0.0533, precision 0.59, recall 0.56, fscore 0.57, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 35: trainloss 0.1072, valloss 0.0502, precision 0.61, recall 0.56, fscore 0.57, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 36: trainloss 0.1070, valloss 0.0517, precision 0.61, recall 0.58, fscore 0.59, kappa 0.19, auroc 0.90\n",
      "\n",
      "epoch 37: trainloss 0.1053, valloss 0.0494, precision 0.65, recall 0.54, fscore 0.56, kappa 0.13, auroc 0.90\n",
      "\n",
      "epoch 38: trainloss 0.1048, valloss 0.0511, precision 0.62, recall 0.58, fscore 0.59, kappa 0.18, auroc 0.90\n",
      "\n",
      "epoch 39: trainloss 0.1045, valloss 0.0491, precision 0.61, recall 0.57, fscore 0.58, kappa 0.17, auroc 0.91\n",
      "\n",
      "epoch 40: trainloss 0.1055, valloss 0.0496, precision 0.60, recall 0.60, fscore 0.60, kappa 0.20, auroc 0.91\n",
      "\n",
      "epoch 41: trainloss 0.1047, valloss 0.0526, precision 0.59, recall 0.63, fscore 0.61, kappa 0.22, auroc 0.91\n",
      "\n",
      "epoch 42: trainloss 0.1033, valloss 0.0496, precision 0.67, recall 0.53, fscore 0.56, kappa 0.11, auroc 0.89\n",
      "\n",
      "epoch 43: trainloss 0.1039, valloss 0.0493, precision 0.66, recall 0.56, fscore 0.59, kappa 0.17, auroc 0.90\n",
      "\n",
      "epoch 44: trainloss 0.1037, valloss 0.0488, precision 0.62, recall 0.58, fscore 0.59, kappa 0.18, auroc 0.91\n",
      "\n",
      "epoch 45: trainloss 0.1039, valloss 0.0501, precision 0.62, recall 0.55, fscore 0.57, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 46: trainloss 0.1026, valloss 0.0479, precision 0.66, recall 0.55, fscore 0.58, kappa 0.16, auroc 0.91\n",
      "\n",
      "epoch 47: trainloss 0.1036, valloss 0.0475, precision 0.63, recall 0.58, fscore 0.60, kappa 0.20, auroc 0.92\n",
      "\n",
      "epoch 48: trainloss 0.1035, valloss 0.0514, precision 0.60, recall 0.60, fscore 0.60, kappa 0.20, auroc 0.91\n",
      "\n",
      "epoch 49: trainloss 0.1033, valloss 0.0490, precision 0.63, recall 0.58, fscore 0.60, kappa 0.20, auroc 0.91\n",
      "\n",
      "epoch 50: trainloss 0.1018, valloss 0.0488, precision 0.62, recall 0.56, fscore 0.58, kappa 0.17, auroc 0.91\n",
      "\n",
      "[[0.2275 0.16   0.1511 0.1447 0.1403 0.135  0.1318 0.129  0.1272 0.1243\n",
      "  0.1219 0.1207 0.1196 0.1187 0.1172 0.1156 0.1146 0.1132 0.1142 0.1141\n",
      "  0.1128 0.1119 0.1107 0.1097 0.1101 0.109  0.109  0.1083 0.1091 0.1075\n",
      "  0.1081 0.1072 0.107  0.1058 0.1072 0.107  0.1053 0.1048 0.1045 0.1055\n",
      "  0.1047 0.1033 0.1039 0.1037 0.1039 0.1026 0.1036 0.1035 0.1033 0.1018]\n",
      " [0.0717 0.1006 0.0741 0.0683 0.0683 0.0657 0.0597 0.0604 0.0642 0.0595\n",
      "  0.0625 0.0599 0.0608 0.0576 0.0578 0.0564 0.0559 0.0563 0.0566 0.0537\n",
      "  0.0546 0.055  0.0542 0.0546 0.0533 0.0515 0.053  0.0545 0.0533 0.0527\n",
      "  0.0497 0.0503 0.0519 0.0533 0.0502 0.0517 0.0494 0.0511 0.0491 0.0496\n",
      "  0.0526 0.0496 0.0493 0.0488 0.0501 0.0479 0.0475 0.0514 0.049  0.0488]\n",
      " [0.54   0.54   0.54   0.55   0.55   0.56   0.55   0.55   0.56   0.56\n",
      "  0.56   0.55   0.56   0.56   0.56   0.56   0.55   0.56   0.55   0.56\n",
      "  0.57   0.57   0.57   0.57   0.57   0.58   0.6    0.57   0.58   0.58\n",
      "  0.61   0.62   0.59   0.59   0.61   0.61   0.65   0.62   0.61   0.6\n",
      "  0.59   0.67   0.66   0.62   0.62   0.66   0.63   0.6    0.63   0.62  ]\n",
      " [0.53   0.58   0.57   0.55   0.56   0.55   0.53   0.53   0.54   0.56\n",
      "  0.53   0.52   0.56   0.52   0.52   0.52   0.51   0.53   0.53   0.53\n",
      "  0.58   0.55   0.57   0.56   0.53   0.55   0.55   0.58   0.58   0.58\n",
      "  0.54   0.55   0.58   0.56   0.56   0.58   0.54   0.58   0.57   0.6\n",
      "  0.63   0.53   0.56   0.58   0.55   0.55   0.58   0.6    0.58   0.56  ]\n",
      " [0.53   0.55   0.55   0.55   0.55   0.55   0.53   0.53   0.55   0.56\n",
      "  0.54   0.53   0.56   0.52   0.53   0.53   0.51   0.54   0.54   0.54\n",
      "  0.58   0.56   0.57   0.56   0.54   0.56   0.56   0.58   0.58   0.58\n",
      "  0.56   0.57   0.59   0.57   0.57   0.59   0.56   0.59   0.58   0.6\n",
      "  0.61   0.56   0.59   0.59   0.57   0.58   0.6    0.6    0.6    0.58  ]\n",
      " [0.07   0.1    0.11   0.1    0.11   0.11   0.07   0.07   0.1    0.11\n",
      "  0.08   0.06   0.11   0.05   0.06   0.07   0.03   0.08   0.08   0.08\n",
      "  0.15   0.12   0.14   0.12   0.08   0.11   0.13   0.15   0.15   0.16\n",
      "  0.12   0.13   0.17   0.15   0.15   0.19   0.13   0.18   0.17   0.2\n",
      "  0.22   0.11   0.17   0.18   0.15   0.16   0.2    0.2    0.2    0.17  ]\n",
      " [0.76   0.72   0.77   0.73   0.79   0.81   0.82   0.83   0.81   0.86\n",
      "  0.81   0.82   0.85   0.84   0.85   0.86   0.85   0.86   0.88   0.88\n",
      "  0.89   0.89   0.9    0.89   0.89   0.9    0.88   0.9    0.9    0.9\n",
      "  0.9    0.89   0.9    0.9    0.9    0.9    0.9    0.9    0.91   0.91\n",
      "  0.91   0.89   0.9    0.91   0.9    0.91   0.92   0.91   0.91   0.91  ]]\n",
      "epoch 1: trainloss 0.2385, valloss 0.0787, precision 0.56, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.62\n",
      "\n",
      "epoch 2: trainloss 0.1786, valloss 0.0702, precision 0.61, recall 0.54, fscore 0.55, kappa 0.11, auroc 0.74\n",
      "\n",
      "epoch 3: trainloss 0.1698, valloss 0.0678, precision 0.60, recall 0.55, fscore 0.56, kappa 0.12, auroc 0.81\n",
      "\n",
      "epoch 4: trainloss 0.1624, valloss 0.0609, precision 0.58, recall 0.53, fscore 0.54, kappa 0.09, auroc 0.85\n",
      "\n",
      "epoch 5: trainloss 0.1542, valloss 0.0643, precision 0.56, recall 0.55, fscore 0.55, kappa 0.10, auroc 0.88\n",
      "\n",
      "epoch 6: trainloss 0.1513, valloss 0.0632, precision 0.54, recall 0.51, fscore 0.51, kappa 0.03, auroc 0.86\n",
      "\n",
      "epoch 7: trainloss 0.1450, valloss 0.0603, precision 0.59, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.87\n",
      "\n",
      "epoch 8: trainloss 0.1443, valloss 0.0925, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.78\n",
      "\n",
      "epoch 9: trainloss 0.1410, valloss 0.0649, precision 0.57, recall 0.56, fscore 0.56, kappa 0.13, auroc 0.88\n",
      "\n",
      "epoch 10: trainloss 0.1369, valloss 0.0615, precision 0.56, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.88\n",
      "\n",
      "epoch 11: trainloss 0.1361, valloss 0.0627, precision 0.55, recall 0.53, fscore 0.53, kappa 0.07, auroc 0.89\n",
      "\n",
      "epoch 12: trainloss 0.1346, valloss 0.0620, precision 0.56, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.87\n",
      "\n",
      "epoch 13: trainloss 0.1342, valloss 0.0590, precision 0.55, recall 0.51, fscore 0.52, kappa 0.04, auroc 0.89\n",
      "\n",
      "epoch 14: trainloss 0.1305, valloss 0.0648, precision 0.57, recall 0.55, fscore 0.56, kappa 0.11, auroc 0.86\n",
      "\n",
      "epoch 15: trainloss 0.1304, valloss 0.0704, precision 0.56, recall 0.58, fscore 0.57, kappa 0.14, auroc 0.88\n",
      "\n",
      "epoch 16: trainloss 0.1305, valloss 0.0592, precision 0.57, recall 0.55, fscore 0.56, kappa 0.12, auroc 0.90\n",
      "\n",
      "epoch 17: trainloss 0.1285, valloss 0.0683, precision 0.55, recall 0.55, fscore 0.55, kappa 0.10, auroc 0.87\n",
      "\n",
      "epoch 18: trainloss 0.1268, valloss 0.0610, precision 0.58, recall 0.57, fscore 0.57, kappa 0.15, auroc 0.89\n",
      "\n",
      "epoch 19: trainloss 0.1292, valloss 0.0653, precision 0.55, recall 0.54, fscore 0.54, kappa 0.08, auroc 0.87\n",
      "\n",
      "epoch 20: trainloss 0.1257, valloss 0.1339, precision 0.51, recall 0.60, fscore 0.51, kappa 0.05, auroc 0.81\n",
      "\n",
      "epoch 21: trainloss 0.1249, valloss 0.0809, precision 0.54, recall 0.60, fscore 0.56, kappa 0.12, auroc 0.87\n",
      "\n",
      "epoch 22: trainloss 0.1251, valloss 0.0781, precision 0.54, recall 0.58, fscore 0.55, kappa 0.10, auroc 0.87\n",
      "\n",
      "epoch 23: trainloss 0.1239, valloss 0.0580, precision 0.57, recall 0.57, fscore 0.57, kappa 0.14, auroc 0.91\n",
      "\n",
      "epoch 24: trainloss 0.1249, valloss 0.0664, precision 0.55, recall 0.55, fscore 0.55, kappa 0.09, auroc 0.88\n",
      "\n",
      "epoch 25: trainloss 0.1229, valloss 0.0657, precision 0.57, recall 0.61, fscore 0.59, kappa 0.17, auroc 0.90\n",
      "\n",
      "epoch 26: trainloss 0.1234, valloss 0.0579, precision 0.57, recall 0.55, fscore 0.55, kappa 0.11, auroc 0.90\n",
      "\n",
      "epoch 27: trainloss 0.1210, valloss 0.0989, precision 0.53, recall 0.62, fscore 0.54, kappa 0.09, auroc 0.86\n",
      "\n",
      "epoch 28: trainloss 0.1217, valloss 0.0704, precision 0.55, recall 0.58, fscore 0.56, kappa 0.12, auroc 0.89\n",
      "\n",
      "epoch 29: trainloss 0.1202, valloss 0.0731, precision 0.54, recall 0.59, fscore 0.55, kappa 0.11, auroc 0.90\n",
      "\n",
      "epoch 30: trainloss 0.1202, valloss 0.0674, precision 0.56, recall 0.60, fscore 0.57, kappa 0.14, auroc 0.89\n",
      "\n",
      "epoch 31: trainloss 0.1226, valloss 0.0900, precision 0.52, recall 0.55, fscore 0.53, kappa 0.06, auroc 0.83\n",
      "\n",
      "epoch 32: trainloss 0.1198, valloss 0.0956, precision 0.53, recall 0.62, fscore 0.54, kappa 0.10, auroc 0.87\n",
      "\n",
      "epoch 33: trainloss 0.1198, valloss 0.0722, precision 0.54, recall 0.57, fscore 0.55, kappa 0.10, auroc 0.87\n",
      "\n",
      "epoch 34: trainloss 0.1191, valloss 0.0708, precision 0.55, recall 0.62, fscore 0.57, kappa 0.15, auroc 0.91\n",
      "\n",
      "epoch 35: trainloss 0.1180, valloss 0.0532, precision 0.62, recall 0.60, fscore 0.61, kappa 0.22, auroc 0.93\n",
      "\n",
      "epoch 36: trainloss 0.1183, valloss 0.0695, precision 0.56, recall 0.62, fscore 0.57, kappa 0.15, auroc 0.90\n",
      "\n",
      "epoch 37: trainloss 0.1172, valloss 0.0481, precision 0.65, recall 0.57, fscore 0.60, kappa 0.19, auroc 0.93\n",
      "\n",
      "epoch 38: trainloss 0.1184, valloss 0.0557, precision 0.58, recall 0.59, fscore 0.58, kappa 0.16, auroc 0.93\n",
      "\n",
      "epoch 39: trainloss 0.1176, valloss 0.0855, precision 0.54, recall 0.64, fscore 0.56, kappa 0.13, auroc 0.90\n",
      "\n",
      "epoch 40: trainloss 0.1165, valloss 0.0542, precision 0.59, recall 0.61, fscore 0.60, kappa 0.20, auroc 0.93\n",
      "\n",
      "epoch 41: trainloss 0.1170, valloss 0.0525, precision 0.62, recall 0.60, fscore 0.61, kappa 0.22, auroc 0.92\n",
      "\n",
      "epoch 42: trainloss 0.1163, valloss 0.0643, precision 0.56, recall 0.60, fscore 0.57, kappa 0.14, auroc 0.91\n",
      "\n",
      "epoch 43: trainloss 0.1164, valloss 0.0606, precision 0.58, recall 0.63, fscore 0.60, kappa 0.20, auroc 0.92\n",
      "\n",
      "epoch 44: trainloss 0.1160, valloss 0.0628, precision 0.58, recall 0.64, fscore 0.60, kappa 0.19, auroc 0.92\n",
      "\n",
      "epoch 45: trainloss 0.1148, valloss 0.0577, precision 0.59, recall 0.63, fscore 0.61, kappa 0.21, auroc 0.93\n",
      "\n",
      "epoch 46: trainloss 0.1147, valloss 0.0545, precision 0.61, recall 0.59, fscore 0.60, kappa 0.19, auroc 0.91\n",
      "\n",
      "epoch 47: trainloss 0.1152, valloss 0.0726, precision 0.56, recall 0.65, fscore 0.58, kappa 0.17, auroc 0.91\n",
      "\n",
      "epoch 48: trainloss 0.1149, valloss 0.0732, precision 0.57, recall 0.66, fscore 0.59, kappa 0.19, auroc 0.92\n",
      "\n",
      "epoch 49: trainloss 0.1148, valloss 0.0608, precision 0.59, recall 0.65, fscore 0.61, kappa 0.22, auroc 0.93\n",
      "\n",
      "epoch 50: trainloss 0.1152, valloss 0.0715, precision 0.57, recall 0.67, fscore 0.59, kappa 0.19, auroc 0.92\n",
      "\n",
      "[[0.2385 0.1786 0.1698 0.1624 0.1542 0.1513 0.145  0.1443 0.141  0.1369\n",
      "  0.1361 0.1346 0.1342 0.1305 0.1304 0.1305 0.1285 0.1268 0.1292 0.1257\n",
      "  0.1249 0.1251 0.1239 0.1249 0.1229 0.1234 0.121  0.1217 0.1202 0.1202\n",
      "  0.1226 0.1198 0.1198 0.1191 0.118  0.1183 0.1172 0.1184 0.1176 0.1165\n",
      "  0.117  0.1163 0.1164 0.116  0.1148 0.1147 0.1152 0.1149 0.1148 0.1152]\n",
      " [0.0787 0.0702 0.0678 0.0609 0.0643 0.0632 0.0603 0.0925 0.0649 0.0615\n",
      "  0.0627 0.062  0.059  0.0648 0.0704 0.0592 0.0683 0.061  0.0653 0.1339\n",
      "  0.0809 0.0781 0.058  0.0664 0.0657 0.0579 0.0989 0.0704 0.0731 0.0674\n",
      "  0.09   0.0956 0.0722 0.0708 0.0532 0.0695 0.0481 0.0557 0.0855 0.0542\n",
      "  0.0525 0.0643 0.0606 0.0628 0.0577 0.0545 0.0726 0.0732 0.0608 0.0715]\n",
      " [0.56   0.61   0.6    0.58   0.56   0.54   0.59   0.56   0.57   0.56\n",
      "  0.55   0.56   0.55   0.57   0.56   0.57   0.55   0.58   0.55   0.51\n",
      "  0.54   0.54   0.57   0.55   0.57   0.57   0.53   0.55   0.54   0.56\n",
      "  0.52   0.53   0.54   0.55   0.62   0.56   0.65   0.58   0.54   0.59\n",
      "  0.62   0.56   0.58   0.58   0.59   0.61   0.56   0.57   0.59   0.57  ]\n",
      " [0.51   0.54   0.55   0.53   0.55   0.51   0.53   0.53   0.56   0.53\n",
      "  0.53   0.53   0.51   0.55   0.58   0.55   0.55   0.57   0.54   0.6\n",
      "  0.6    0.58   0.57   0.55   0.61   0.55   0.62   0.58   0.59   0.6\n",
      "  0.55   0.62   0.57   0.62   0.6    0.62   0.57   0.59   0.64   0.61\n",
      "  0.6    0.6    0.63   0.64   0.63   0.59   0.65   0.66   0.65   0.67  ]\n",
      " [0.52   0.55   0.56   0.54   0.55   0.51   0.54   0.54   0.56   0.53\n",
      "  0.53   0.54   0.52   0.56   0.57   0.56   0.55   0.57   0.54   0.51\n",
      "  0.56   0.55   0.57   0.55   0.59   0.55   0.54   0.56   0.55   0.57\n",
      "  0.53   0.54   0.55   0.57   0.61   0.57   0.6    0.58   0.56   0.6\n",
      "  0.61   0.57   0.6    0.6    0.61   0.6    0.58   0.59   0.61   0.59  ]\n",
      " [0.04   0.11   0.12   0.09   0.1    0.03   0.08   0.08   0.13   0.07\n",
      "  0.07   0.08   0.04   0.11   0.14   0.12   0.1    0.15   0.08   0.05\n",
      "  0.12   0.1    0.14   0.09   0.17   0.11   0.09   0.12   0.11   0.14\n",
      "  0.06   0.1    0.1    0.15   0.22   0.15   0.19   0.16   0.13   0.2\n",
      "  0.22   0.14   0.2    0.19   0.21   0.19   0.17   0.19   0.22   0.19  ]\n",
      " [0.62   0.74   0.81   0.85   0.88   0.86   0.87   0.78   0.88   0.88\n",
      "  0.89   0.87   0.89   0.86   0.88   0.9    0.87   0.89   0.87   0.81\n",
      "  0.87   0.87   0.91   0.88   0.9    0.9    0.86   0.89   0.9    0.89\n",
      "  0.83   0.87   0.87   0.91   0.93   0.9    0.93   0.93   0.9    0.93\n",
      "  0.92   0.91   0.92   0.92   0.93   0.91   0.91   0.92   0.93   0.92  ]]\n",
      "epoch 1: trainloss 0.2529, valloss 0.2994, precision 0.92, recall 0.54, fscore 0.54, kappa 0.13, auroc 0.90\n",
      "\n",
      "epoch 2: trainloss 0.1346, valloss 0.5639, precision 0.88, recall 0.59, fscore 0.62, kappa 0.27, auroc 0.77\n",
      "\n",
      "epoch 3: trainloss 0.1174, valloss 0.2643, precision 0.87, recall 0.76, fscore 0.80, kappa 0.61, auroc 0.90\n",
      "\n",
      "epoch 4: trainloss 0.1140, valloss 0.2344, precision 0.85, recall 0.84, fscore 0.84, kappa 0.68, auroc 0.92\n",
      "\n",
      "epoch 5: trainloss 0.1102, valloss 0.3612, precision 0.67, recall 0.81, fscore 0.69, kappa 0.40, auroc 0.87\n",
      "\n",
      "epoch 6: trainloss 0.1094, valloss 0.2517, precision 0.90, recall 0.62, fscore 0.66, kappa 0.34, auroc 0.93\n",
      "\n",
      "epoch 7: trainloss 0.1083, valloss 0.2726, precision 0.72, recall 0.86, fscore 0.75, kappa 0.52, auroc 0.92\n",
      "\n",
      "epoch 8: trainloss 0.1075, valloss 0.2688, precision 0.90, recall 0.59, fscore 0.63, kappa 0.28, auroc 0.92\n",
      "\n",
      "epoch 9: trainloss 0.1031, valloss 0.2459, precision 0.73, recall 0.86, fscore 0.77, kappa 0.54, auroc 0.92\n",
      "\n",
      "epoch 10: trainloss 0.1048, valloss 0.2829, precision 0.68, recall 0.84, fscore 0.71, kappa 0.44, auroc 0.91\n",
      "\n",
      "epoch 11: trainloss 0.1047, valloss 0.3703, precision 0.84, recall 0.53, fscore 0.52, kappa 0.09, auroc 0.85\n",
      "\n",
      "epoch 12: trainloss 0.1009, valloss 0.2672, precision 0.84, recall 0.64, fscore 0.69, kappa 0.39, auroc 0.92\n",
      "\n",
      "epoch 13: trainloss 0.0972, valloss 0.2311, precision 0.85, recall 0.70, fscore 0.74, kappa 0.49, auroc 0.93\n",
      "\n",
      "epoch 14: trainloss 0.0965, valloss 0.2064, precision 0.80, recall 0.86, fscore 0.82, kappa 0.65, auroc 0.94\n",
      "\n",
      "epoch 15: trainloss 0.0941, valloss 0.2117, precision 0.77, recall 0.87, fscore 0.81, kappa 0.62, auroc 0.94\n",
      "\n",
      "epoch 16: trainloss 0.0939, valloss 0.2476, precision 0.70, recall 0.86, fscore 0.73, kappa 0.48, auroc 0.92\n",
      "\n",
      "epoch 17: trainloss 0.0914, valloss 0.2056, precision 0.81, recall 0.83, fscore 0.82, kappa 0.64, auroc 0.94\n",
      "\n",
      "epoch 18: trainloss 0.0891, valloss 0.2141, precision 0.76, recall 0.86, fscore 0.80, kappa 0.60, auroc 0.93\n",
      "\n",
      "epoch 19: trainloss 0.0905, valloss 0.2256, precision 0.78, recall 0.82, fscore 0.79, kappa 0.59, auroc 0.92\n",
      "\n",
      "epoch 20: trainloss 0.0896, valloss 0.2008, precision 0.78, recall 0.86, fscore 0.81, kappa 0.62, auroc 0.94\n",
      "\n",
      "epoch 21: trainloss 0.0839, valloss 0.2048, precision 0.77, recall 0.85, fscore 0.80, kappa 0.60, auroc 0.94\n",
      "\n",
      "epoch 22: trainloss 0.0849, valloss 0.2164, precision 0.75, recall 0.86, fscore 0.78, kappa 0.57, auroc 0.93\n",
      "\n",
      "epoch 23: trainloss 0.0847, valloss 0.1992, precision 0.77, recall 0.85, fscore 0.80, kappa 0.61, auroc 0.94\n",
      "\n",
      "epoch 24: trainloss 0.0848, valloss 0.2293, precision 0.74, recall 0.83, fscore 0.77, kappa 0.55, auroc 0.92\n",
      "\n",
      "epoch 25: trainloss 0.0815, valloss 0.2666, precision 0.72, recall 0.80, fscore 0.75, kappa 0.50, auroc 0.89\n",
      "\n",
      "epoch 26: trainloss 0.0803, valloss 0.2109, precision 0.83, recall 0.77, fscore 0.79, kappa 0.59, auroc 0.94\n",
      "\n",
      "epoch 27: trainloss 0.0804, valloss 0.2109, precision 0.74, recall 0.86, fscore 0.78, kappa 0.56, auroc 0.93\n",
      "\n",
      "epoch 28: trainloss 0.0765, valloss 0.1944, precision 0.78, recall 0.85, fscore 0.81, kappa 0.62, auroc 0.94\n",
      "\n",
      "epoch 29: trainloss 0.0771, valloss 0.2055, precision 0.75, recall 0.87, fscore 0.79, kappa 0.58, auroc 0.94\n",
      "\n",
      "epoch 30: trainloss 0.0781, valloss 0.2034, precision 0.77, recall 0.83, fscore 0.80, kappa 0.59, auroc 0.94\n",
      "\n",
      "epoch 31: trainloss 0.0769, valloss 0.1877, precision 0.81, recall 0.84, fscore 0.82, kappa 0.64, auroc 0.95\n",
      "\n",
      "epoch 32: trainloss 0.0747, valloss 0.2097, precision 0.74, recall 0.87, fscore 0.78, kappa 0.57, auroc 0.94\n",
      "\n",
      "epoch 33: trainloss 0.0761, valloss 0.1946, precision 0.77, recall 0.86, fscore 0.80, kappa 0.61, auroc 0.94\n",
      "\n",
      "epoch 34: trainloss 0.0768, valloss 0.2158, precision 0.73, recall 0.87, fscore 0.77, kappa 0.54, auroc 0.94\n",
      "\n",
      "epoch 35: trainloss 0.0752, valloss 0.2201, precision 0.73, recall 0.85, fscore 0.77, kappa 0.54, auroc 0.93\n",
      "\n",
      "epoch 36: trainloss 0.0744, valloss 0.2067, precision 0.74, recall 0.87, fscore 0.78, kappa 0.57, auroc 0.94\n",
      "\n",
      "epoch 37: trainloss 0.0738, valloss 0.2382, precision 0.73, recall 0.86, fscore 0.77, kappa 0.54, auroc 0.93\n",
      "\n",
      "epoch 38: trainloss 0.0738, valloss 0.1914, precision 0.77, recall 0.86, fscore 0.81, kappa 0.61, auroc 0.94\n",
      "\n",
      "epoch 39: trainloss 0.0722, valloss 0.2061, precision 0.76, recall 0.84, fscore 0.79, kappa 0.59, auroc 0.93\n",
      "\n",
      "epoch 40: trainloss 0.0755, valloss 0.2082, precision 0.76, recall 0.85, fscore 0.79, kappa 0.59, auroc 0.93\n",
      "\n",
      "epoch 41: trainloss 0.0722, valloss 0.1910, precision 0.78, recall 0.85, fscore 0.81, kappa 0.62, auroc 0.94\n",
      "\n",
      "epoch 42: trainloss 0.0731, valloss 0.2228, precision 0.74, recall 0.85, fscore 0.78, kappa 0.56, auroc 0.93\n",
      "\n",
      "epoch 43: trainloss 0.0719, valloss 0.1953, precision 0.79, recall 0.84, fscore 0.81, kappa 0.62, auroc 0.94\n",
      "\n",
      "epoch 44: trainloss 0.0718, valloss 0.1994, precision 0.76, recall 0.85, fscore 0.80, kappa 0.59, auroc 0.94\n",
      "\n",
      "epoch 45: trainloss 0.0716, valloss 0.2223, precision 0.74, recall 0.87, fscore 0.77, kappa 0.56, auroc 0.94\n",
      "\n",
      "epoch 46: trainloss 0.0688, valloss 0.1938, precision 0.76, recall 0.86, fscore 0.80, kappa 0.60, auroc 0.94\n",
      "\n",
      "epoch 47: trainloss 0.0721, valloss 0.2038, precision 0.75, recall 0.87, fscore 0.79, kappa 0.58, auroc 0.94\n",
      "\n",
      "epoch 48: trainloss 0.0700, valloss 0.1975, precision 0.76, recall 0.86, fscore 0.80, kappa 0.59, auroc 0.94\n",
      "\n",
      "epoch 49: trainloss 0.0696, valloss 0.1958, precision 0.77, recall 0.86, fscore 0.81, kappa 0.61, auroc 0.94\n",
      "\n",
      "epoch 50: trainloss 0.0681, valloss 0.1961, precision 0.77, recall 0.86, fscore 0.81, kappa 0.62, auroc 0.94\n",
      "\n",
      "[[0.2529 0.1346 0.1174 0.114  0.1102 0.1094 0.1083 0.1075 0.1031 0.1048\n",
      "  0.1047 0.1009 0.0972 0.0965 0.0941 0.0939 0.0914 0.0891 0.0905 0.0896\n",
      "  0.0839 0.0849 0.0847 0.0848 0.0815 0.0803 0.0804 0.0765 0.0771 0.0781\n",
      "  0.0769 0.0747 0.0761 0.0768 0.0752 0.0744 0.0738 0.0738 0.0722 0.0755\n",
      "  0.0722 0.0731 0.0719 0.0718 0.0716 0.0688 0.0721 0.07   0.0696 0.0681]\n",
      " [0.2994 0.5639 0.2643 0.2344 0.3612 0.2517 0.2726 0.2688 0.2459 0.2829\n",
      "  0.3703 0.2672 0.2311 0.2064 0.2117 0.2476 0.2056 0.2141 0.2256 0.2008\n",
      "  0.2048 0.2164 0.1992 0.2293 0.2666 0.2109 0.2109 0.1944 0.2055 0.2034\n",
      "  0.1877 0.2097 0.1946 0.2158 0.2201 0.2067 0.2382 0.1914 0.2061 0.2082\n",
      "  0.191  0.2228 0.1953 0.1994 0.2223 0.1938 0.2038 0.1975 0.1958 0.1961]\n",
      " [0.92   0.88   0.87   0.85   0.67   0.9    0.72   0.9    0.73   0.68\n",
      "  0.84   0.84   0.85   0.8    0.77   0.7    0.81   0.76   0.78   0.78\n",
      "  0.77   0.75   0.77   0.74   0.72   0.83   0.74   0.78   0.75   0.77\n",
      "  0.81   0.74   0.77   0.73   0.73   0.74   0.73   0.77   0.76   0.76\n",
      "  0.78   0.74   0.79   0.76   0.74   0.76   0.75   0.76   0.77   0.77  ]\n",
      " [0.54   0.59   0.76   0.84   0.81   0.62   0.86   0.59   0.86   0.84\n",
      "  0.53   0.64   0.7    0.86   0.87   0.86   0.83   0.86   0.82   0.86\n",
      "  0.85   0.86   0.85   0.83   0.8    0.77   0.86   0.85   0.87   0.83\n",
      "  0.84   0.87   0.86   0.87   0.85   0.87   0.86   0.86   0.84   0.85\n",
      "  0.85   0.85   0.84   0.85   0.87   0.86   0.87   0.86   0.86   0.86  ]\n",
      " [0.54   0.62   0.8    0.84   0.69   0.66   0.75   0.63   0.77   0.71\n",
      "  0.52   0.69   0.74   0.82   0.81   0.73   0.82   0.8    0.79   0.81\n",
      "  0.8    0.78   0.8    0.77   0.75   0.79   0.78   0.81   0.79   0.8\n",
      "  0.82   0.78   0.8    0.77   0.77   0.78   0.77   0.81   0.79   0.79\n",
      "  0.81   0.78   0.81   0.8    0.77   0.8    0.79   0.8    0.81   0.81  ]\n",
      " [0.13   0.27   0.61   0.68   0.4    0.34   0.52   0.28   0.54   0.44\n",
      "  0.09   0.39   0.49   0.65   0.62   0.48   0.64   0.6    0.59   0.62\n",
      "  0.6    0.57   0.61   0.55   0.5    0.59   0.56   0.62   0.58   0.59\n",
      "  0.64   0.57   0.61   0.54   0.54   0.57   0.54   0.61   0.59   0.59\n",
      "  0.62   0.56   0.62   0.59   0.56   0.6    0.58   0.59   0.61   0.62  ]\n",
      " [0.9    0.77   0.9    0.92   0.87   0.93   0.92   0.92   0.92   0.91\n",
      "  0.85   0.92   0.93   0.94   0.94   0.92   0.94   0.93   0.92   0.94\n",
      "  0.94   0.93   0.94   0.92   0.89   0.94   0.93   0.94   0.94   0.94\n",
      "  0.95   0.94   0.94   0.94   0.93   0.94   0.93   0.94   0.93   0.93\n",
      "  0.94   0.93   0.94   0.94   0.94   0.94   0.94   0.94   0.94   0.94  ]]\n",
      "epoch 1: trainloss 0.2261, valloss 0.4421, precision 0.77, recall 0.51, fscore 0.49, kappa 0.05, auroc 0.70\n",
      "\n",
      "epoch 2: trainloss 0.1543, valloss 0.3454, precision 0.78, recall 0.61, fscore 0.65, kappa 0.31, auroc 0.86\n",
      "\n",
      "epoch 3: trainloss 0.1469, valloss 0.4858, precision 0.78, recall 0.58, fscore 0.61, kappa 0.24, auroc 0.75\n",
      "\n",
      "epoch 4: trainloss 0.1401, valloss 0.3227, precision 0.76, recall 0.67, fscore 0.70, kappa 0.40, auroc 0.85\n",
      "\n",
      "epoch 5: trainloss 0.1338, valloss 0.3836, precision 0.81, recall 0.58, fscore 0.61, kappa 0.25, auroc 0.83\n",
      "\n",
      "epoch 6: trainloss 0.1316, valloss 0.3694, precision 0.80, recall 0.61, fscore 0.65, kappa 0.31, auroc 0.84\n",
      "\n",
      "epoch 7: trainloss 0.1278, valloss 0.3538, precision 0.81, recall 0.63, fscore 0.66, kappa 0.34, auroc 0.86\n",
      "\n",
      "epoch 8: trainloss 0.1236, valloss 0.3000, precision 0.78, recall 0.67, fscore 0.71, kappa 0.42, auroc 0.88\n",
      "\n",
      "epoch 9: trainloss 0.1228, valloss 0.4307, precision 0.86, recall 0.55, fscore 0.55, kappa 0.15, auroc 0.83\n",
      "\n",
      "epoch 10: trainloss 0.1214, valloss 0.3921, precision 0.82, recall 0.60, fscore 0.63, kappa 0.27, auroc 0.87\n",
      "\n",
      "epoch 11: trainloss 0.1196, valloss 0.3425, precision 0.83, recall 0.63, fscore 0.67, kappa 0.36, auroc 0.89\n",
      "\n",
      "epoch 12: trainloss 0.1181, valloss 0.3076, precision 0.80, recall 0.69, fscore 0.72, kappa 0.45, auroc 0.89\n",
      "\n",
      "epoch 13: trainloss 0.1175, valloss 0.3295, precision 0.77, recall 0.66, fscore 0.69, kappa 0.39, auroc 0.86\n",
      "\n",
      "epoch 14: trainloss 0.1155, valloss 0.3373, precision 0.81, recall 0.62, fscore 0.65, kappa 0.32, auroc 0.88\n",
      "\n",
      "epoch 15: trainloss 0.1150, valloss 0.3118, precision 0.78, recall 0.68, fscore 0.71, kappa 0.43, auroc 0.87\n",
      "\n",
      "epoch 16: trainloss 0.1141, valloss 0.2769, precision 0.82, recall 0.69, fscore 0.73, kappa 0.47, auroc 0.89\n",
      "\n",
      "epoch 17: trainloss 0.1123, valloss 0.3112, precision 0.71, recall 0.68, fscore 0.69, kappa 0.39, auroc 0.85\n",
      "\n",
      "epoch 18: trainloss 0.1121, valloss 0.3911, precision 0.83, recall 0.59, fscore 0.62, kappa 0.27, auroc 0.85\n",
      "\n",
      "epoch 19: trainloss 0.1107, valloss 0.3286, precision 0.75, recall 0.65, fscore 0.68, kappa 0.36, auroc 0.85\n",
      "\n",
      "epoch 20: trainloss 0.1119, valloss 0.3303, precision 0.82, recall 0.63, fscore 0.66, kappa 0.34, auroc 0.89\n",
      "\n",
      "epoch 21: trainloss 0.1089, valloss 0.3359, precision 0.79, recall 0.66, fscore 0.70, kappa 0.40, auroc 0.87\n",
      "\n",
      "epoch 22: trainloss 0.1101, valloss 0.3168, precision 0.76, recall 0.68, fscore 0.71, kappa 0.42, auroc 0.87\n",
      "\n",
      "epoch 23: trainloss 0.1093, valloss 0.3079, precision 0.82, recall 0.68, fscore 0.72, kappa 0.44, auroc 0.89\n",
      "\n",
      "epoch 24: trainloss 0.1094, valloss 0.4308, precision 0.79, recall 0.55, fscore 0.55, kappa 0.14, auroc 0.85\n",
      "\n",
      "epoch 25: trainloss 0.1086, valloss 0.3748, precision 0.83, recall 0.59, fscore 0.62, kappa 0.27, auroc 0.88\n",
      "\n",
      "epoch 26: trainloss 0.1088, valloss 0.3492, precision 0.81, recall 0.62, fscore 0.66, kappa 0.34, auroc 0.88\n",
      "\n",
      "epoch 27: trainloss 0.1068, valloss 0.3073, precision 0.82, recall 0.66, fscore 0.70, kappa 0.41, auroc 0.90\n",
      "\n",
      "epoch 28: trainloss 0.1063, valloss 0.3368, precision 0.82, recall 0.63, fscore 0.67, kappa 0.36, auroc 0.88\n",
      "\n",
      "epoch 29: trainloss 0.1063, valloss 0.3401, precision 0.81, recall 0.61, fscore 0.65, kappa 0.32, auroc 0.89\n",
      "\n",
      "epoch 30: trainloss 0.1063, valloss 0.3266, precision 0.78, recall 0.67, fscore 0.71, kappa 0.42, auroc 0.87\n",
      "\n",
      "epoch 31: trainloss 0.1059, valloss 0.3462, precision 0.75, recall 0.65, fscore 0.68, kappa 0.37, auroc 0.86\n",
      "\n",
      "epoch 32: trainloss 0.1056, valloss 0.3032, precision 0.82, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.90\n",
      "\n",
      "epoch 33: trainloss 0.1050, valloss 0.3433, precision 0.79, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.86\n",
      "\n",
      "epoch 34: trainloss 0.1042, valloss 0.3400, precision 0.81, recall 0.62, fscore 0.65, kappa 0.32, auroc 0.89\n",
      "\n",
      "epoch 35: trainloss 0.1048, valloss 0.3652, precision 0.80, recall 0.61, fscore 0.64, kappa 0.31, auroc 0.88\n",
      "\n",
      "epoch 36: trainloss 0.1025, valloss 0.3202, precision 0.81, recall 0.67, fscore 0.71, kappa 0.43, auroc 0.90\n",
      "\n",
      "epoch 37: trainloss 0.1032, valloss 0.4030, precision 0.78, recall 0.61, fscore 0.64, kappa 0.30, auroc 0.85\n",
      "\n",
      "epoch 38: trainloss 0.1035, valloss 0.3474, precision 0.81, recall 0.63, fscore 0.67, kappa 0.36, auroc 0.89\n",
      "\n",
      "epoch 39: trainloss 0.1026, valloss 0.3462, precision 0.73, recall 0.64, fscore 0.67, kappa 0.34, auroc 0.87\n",
      "\n",
      "epoch 40: trainloss 0.1021, valloss 0.3867, precision 0.80, recall 0.61, fscore 0.65, kappa 0.31, auroc 0.87\n",
      "\n",
      "epoch 41: trainloss 0.1041, valloss 0.3457, precision 0.79, recall 0.62, fscore 0.65, kappa 0.32, auroc 0.88\n",
      "\n",
      "epoch 42: trainloss 0.1015, valloss 0.3525, precision 0.81, recall 0.61, fscore 0.64, kappa 0.30, auroc 0.88\n",
      "\n",
      "epoch 43: trainloss 0.1013, valloss 0.3221, precision 0.78, recall 0.64, fscore 0.68, kappa 0.37, auroc 0.88\n",
      "\n",
      "epoch 44: trainloss 0.1009, valloss 0.3299, precision 0.78, recall 0.68, fscore 0.71, kappa 0.43, auroc 0.89\n",
      "\n",
      "epoch 45: trainloss 0.1000, valloss 0.3508, precision 0.79, recall 0.65, fscore 0.68, kappa 0.38, auroc 0.87\n",
      "\n",
      "epoch 46: trainloss 0.1013, valloss 0.3676, precision 0.76, recall 0.63, fscore 0.66, kappa 0.34, auroc 0.86\n",
      "\n",
      "epoch 47: trainloss 0.1013, valloss 0.3080, precision 0.82, recall 0.68, fscore 0.72, kappa 0.45, auroc 0.90\n",
      "\n",
      "epoch 48: trainloss 0.1001, valloss 0.3134, precision 0.81, recall 0.68, fscore 0.72, kappa 0.45, auroc 0.89\n",
      "\n",
      "epoch 49: trainloss 0.0996, valloss 0.3203, precision 0.78, recall 0.65, fscore 0.69, kappa 0.38, auroc 0.89\n",
      "\n",
      "epoch 50: trainloss 0.1000, valloss 0.3519, precision 0.80, recall 0.64, fscore 0.67, kappa 0.36, auroc 0.89\n",
      "\n",
      "[[0.2261 0.1543 0.1469 0.1401 0.1338 0.1316 0.1278 0.1236 0.1228 0.1214\n",
      "  0.1196 0.1181 0.1175 0.1155 0.115  0.1141 0.1123 0.1121 0.1107 0.1119\n",
      "  0.1089 0.1101 0.1093 0.1094 0.1086 0.1088 0.1068 0.1063 0.1063 0.1063\n",
      "  0.1059 0.1056 0.105  0.1042 0.1048 0.1025 0.1032 0.1035 0.1026 0.1021\n",
      "  0.1041 0.1015 0.1013 0.1009 0.1    0.1013 0.1013 0.1001 0.0996 0.1   ]\n",
      " [0.4421 0.3454 0.4858 0.3227 0.3836 0.3694 0.3538 0.3    0.4307 0.3921\n",
      "  0.3425 0.3076 0.3295 0.3373 0.3118 0.2769 0.3112 0.3911 0.3286 0.3303\n",
      "  0.3359 0.3168 0.3079 0.4308 0.3748 0.3492 0.3073 0.3368 0.3401 0.3266\n",
      "  0.3462 0.3032 0.3433 0.34   0.3652 0.3202 0.403  0.3474 0.3462 0.3867\n",
      "  0.3457 0.3525 0.3221 0.3299 0.3508 0.3676 0.308  0.3134 0.3203 0.3519]\n",
      " [0.77   0.78   0.78   0.76   0.81   0.8    0.81   0.78   0.86   0.82\n",
      "  0.83   0.8    0.77   0.81   0.78   0.82   0.71   0.83   0.75   0.82\n",
      "  0.79   0.76   0.82   0.79   0.83   0.81   0.82   0.82   0.81   0.78\n",
      "  0.75   0.82   0.79   0.81   0.8    0.81   0.78   0.81   0.73   0.8\n",
      "  0.79   0.81   0.78   0.78   0.79   0.76   0.82   0.81   0.78   0.8   ]\n",
      " [0.51   0.61   0.58   0.67   0.58   0.61   0.63   0.67   0.55   0.6\n",
      "  0.63   0.69   0.66   0.62   0.68   0.69   0.68   0.59   0.65   0.63\n",
      "  0.66   0.68   0.68   0.55   0.59   0.62   0.66   0.63   0.61   0.67\n",
      "  0.65   0.67   0.67   0.62   0.61   0.67   0.61   0.63   0.64   0.61\n",
      "  0.62   0.61   0.64   0.68   0.65   0.63   0.68   0.68   0.65   0.64  ]\n",
      " [0.49   0.65   0.61   0.7    0.61   0.65   0.66   0.71   0.55   0.63\n",
      "  0.67   0.72   0.69   0.65   0.71   0.73   0.69   0.62   0.68   0.66\n",
      "  0.7    0.71   0.72   0.55   0.62   0.66   0.7    0.67   0.65   0.71\n",
      "  0.68   0.71   0.71   0.65   0.64   0.71   0.64   0.67   0.67   0.65\n",
      "  0.65   0.64   0.68   0.71   0.68   0.66   0.72   0.72   0.69   0.67  ]\n",
      " [0.05   0.31   0.24   0.4    0.25   0.31   0.34   0.42   0.15   0.27\n",
      "  0.36   0.45   0.39   0.32   0.43   0.47   0.39   0.27   0.36   0.34\n",
      "  0.4    0.42   0.44   0.14   0.27   0.34   0.41   0.36   0.32   0.42\n",
      "  0.37   0.43   0.43   0.32   0.31   0.43   0.3    0.36   0.34   0.31\n",
      "  0.32   0.3    0.37   0.43   0.38   0.34   0.45   0.45   0.38   0.36  ]\n",
      " [0.7    0.86   0.75   0.85   0.83   0.84   0.86   0.88   0.83   0.87\n",
      "  0.89   0.89   0.86   0.88   0.87   0.89   0.85   0.85   0.85   0.89\n",
      "  0.87   0.87   0.89   0.85   0.88   0.88   0.9    0.88   0.89   0.87\n",
      "  0.86   0.9    0.86   0.89   0.88   0.9    0.85   0.89   0.87   0.87\n",
      "  0.88   0.88   0.88   0.89   0.87   0.86   0.9    0.89   0.89   0.89  ]]\n",
      "epoch 1: trainloss 0.1714, valloss 0.7717, precision 0.38, recall 0.50, fscore 0.43, kappa -0.00, auroc 0.87\n",
      "\n",
      "epoch 2: trainloss 0.1055, valloss 0.4431, precision 0.86, recall 0.66, fscore 0.69, kappa 0.40, auroc 0.88\n",
      "\n",
      "epoch 3: trainloss 0.1002, valloss 0.4785, precision 0.86, recall 0.61, fscore 0.63, kappa 0.31, auroc 0.89\n",
      "\n",
      "epoch 4: trainloss 0.0975, valloss 0.4765, precision 0.86, recall 0.57, fscore 0.56, kappa 0.20, auroc 0.90\n",
      "\n",
      "epoch 5: trainloss 0.0972, valloss 0.4372, precision 0.85, recall 0.68, fscore 0.71, kappa 0.44, auroc 0.90\n",
      "\n",
      "epoch 6: trainloss 0.0932, valloss 0.4204, precision 0.82, recall 0.77, fscore 0.79, kappa 0.58, auroc 0.89\n",
      "\n",
      "epoch 7: trainloss 0.0896, valloss 0.4867, precision 0.81, recall 0.72, fscore 0.74, kappa 0.50, auroc 0.86\n",
      "\n",
      "epoch 8: trainloss 0.0842, valloss 0.4009, precision 0.81, recall 0.80, fscore 0.81, kappa 0.61, auroc 0.90\n",
      "\n",
      "epoch 9: trainloss 0.0824, valloss 0.6025, precision 0.72, recall 0.68, fscore 0.70, kappa 0.40, auroc 0.78\n",
      "\n",
      "epoch 10: trainloss 0.0823, valloss 0.3935, precision 0.73, recall 0.80, fscore 0.74, kappa 0.49, auroc 0.89\n",
      "\n",
      "epoch 11: trainloss 0.0814, valloss 0.4230, precision 0.78, recall 0.73, fscore 0.75, kappa 0.50, auroc 0.88\n",
      "\n",
      "epoch 12: trainloss 0.0784, valloss 0.3999, precision 0.70, recall 0.78, fscore 0.70, kappa 0.43, auroc 0.87\n",
      "\n",
      "epoch 13: trainloss 0.0757, valloss 0.4048, precision 0.75, recall 0.79, fscore 0.77, kappa 0.53, auroc 0.87\n",
      "\n",
      "epoch 14: trainloss 0.0763, valloss 0.4111, precision 0.70, recall 0.78, fscore 0.68, kappa 0.41, auroc 0.87\n",
      "\n",
      "epoch 15: trainloss 0.0730, valloss 0.3620, precision 0.77, recall 0.80, fscore 0.78, kappa 0.56, auroc 0.89\n",
      "\n",
      "epoch 16: trainloss 0.0731, valloss 0.3873, precision 0.72, recall 0.80, fscore 0.72, kappa 0.47, auroc 0.87\n",
      "\n",
      "epoch 17: trainloss 0.0698, valloss 0.3737, precision 0.75, recall 0.79, fscore 0.76, kappa 0.53, auroc 0.87\n",
      "\n",
      "epoch 18: trainloss 0.0702, valloss 0.3773, precision 0.73, recall 0.80, fscore 0.75, kappa 0.51, auroc 0.87\n",
      "\n",
      "epoch 19: trainloss 0.0684, valloss 0.4191, precision 0.71, recall 0.78, fscore 0.71, kappa 0.44, auroc 0.85\n",
      "\n",
      "epoch 20: trainloss 0.0671, valloss 0.4137, precision 0.72, recall 0.78, fscore 0.73, kappa 0.47, auroc 0.86\n",
      "\n",
      "epoch 21: trainloss 0.0674, valloss 0.4437, precision 0.71, recall 0.78, fscore 0.71, kappa 0.44, auroc 0.85\n",
      "\n",
      "epoch 22: trainloss 0.0670, valloss 0.4778, precision 0.70, recall 0.77, fscore 0.71, kappa 0.44, auroc 0.85\n",
      "\n",
      "epoch 23: trainloss 0.0647, valloss 0.3719, precision 0.74, recall 0.79, fscore 0.76, kappa 0.52, auroc 0.87\n",
      "\n",
      "epoch 24: trainloss 0.0668, valloss 0.4010, precision 0.71, recall 0.78, fscore 0.71, kappa 0.45, auroc 0.87\n",
      "\n",
      "epoch 25: trainloss 0.0649, valloss 0.4153, precision 0.71, recall 0.78, fscore 0.72, kappa 0.46, auroc 0.86\n",
      "\n",
      "epoch 26: trainloss 0.0651, valloss 0.4549, precision 0.69, recall 0.75, fscore 0.70, kappa 0.41, auroc 0.83\n",
      "\n",
      "epoch 27: trainloss 0.0637, valloss 0.3596, precision 0.76, recall 0.80, fscore 0.77, kappa 0.55, auroc 0.89\n",
      "\n",
      "epoch 28: trainloss 0.0637, valloss 0.3897, precision 0.72, recall 0.78, fscore 0.73, kappa 0.46, auroc 0.86\n",
      "\n",
      "epoch 29: trainloss 0.0621, valloss 0.3878, precision 0.78, recall 0.77, fscore 0.78, kappa 0.55, auroc 0.88\n",
      "\n",
      "epoch 30: trainloss 0.0630, valloss 0.4018, precision 0.72, recall 0.78, fscore 0.74, kappa 0.48, auroc 0.87\n",
      "\n",
      "epoch 31: trainloss 0.0625, valloss 0.4327, precision 0.70, recall 0.75, fscore 0.72, kappa 0.44, auroc 0.84\n",
      "\n",
      "epoch 32: trainloss 0.0626, valloss 0.4285, precision 0.71, recall 0.78, fscore 0.72, kappa 0.46, auroc 0.86\n",
      "\n",
      "epoch 33: trainloss 0.0621, valloss 0.3902, precision 0.72, recall 0.79, fscore 0.73, kappa 0.48, auroc 0.87\n",
      "\n",
      "epoch 34: trainloss 0.0622, valloss 0.4301, precision 0.70, recall 0.77, fscore 0.70, kappa 0.41, auroc 0.85\n",
      "\n",
      "epoch 35: trainloss 0.0604, valloss 0.3652, precision 0.74, recall 0.80, fscore 0.76, kappa 0.53, auroc 0.89\n",
      "\n",
      "epoch 36: trainloss 0.0612, valloss 0.4013, precision 0.72, recall 0.79, fscore 0.73, kappa 0.47, auroc 0.86\n",
      "\n",
      "epoch 37: trainloss 0.0613, valloss 0.4009, precision 0.74, recall 0.77, fscore 0.75, kappa 0.51, auroc 0.87\n",
      "\n",
      "epoch 38: trainloss 0.0613, valloss 0.4198, precision 0.72, recall 0.78, fscore 0.73, kappa 0.46, auroc 0.87\n",
      "\n",
      "epoch 39: trainloss 0.0603, valloss 0.4415, precision 0.71, recall 0.75, fscore 0.72, kappa 0.45, auroc 0.85\n",
      "\n",
      "epoch 40: trainloss 0.0591, valloss 0.4459, precision 0.71, recall 0.78, fscore 0.71, kappa 0.45, auroc 0.86\n",
      "\n",
      "epoch 41: trainloss 0.0599, valloss 0.4008, precision 0.71, recall 0.78, fscore 0.72, kappa 0.46, auroc 0.86\n",
      "\n",
      "epoch 42: trainloss 0.0594, valloss 0.4566, precision 0.70, recall 0.78, fscore 0.70, kappa 0.43, auroc 0.86\n",
      "\n",
      "epoch 43: trainloss 0.0589, valloss 0.4630, precision 0.71, recall 0.78, fscore 0.71, kappa 0.45, auroc 0.86\n",
      "\n",
      "epoch 44: trainloss 0.0602, valloss 0.4348, precision 0.70, recall 0.78, fscore 0.70, kappa 0.43, auroc 0.88\n",
      "\n",
      "epoch 45: trainloss 0.0588, valloss 0.3682, precision 0.76, recall 0.81, fscore 0.77, kappa 0.55, auroc 0.89\n",
      "\n",
      "epoch 46: trainloss 0.0577, valloss 0.4396, precision 0.71, recall 0.75, fscore 0.72, kappa 0.44, auroc 0.84\n",
      "\n",
      "epoch 47: trainloss 0.0596, valloss 0.4105, precision 0.73, recall 0.80, fscore 0.74, kappa 0.49, auroc 0.88\n",
      "\n",
      "epoch 48: trainloss 0.0588, valloss 0.4216, precision 0.73, recall 0.79, fscore 0.74, kappa 0.49, auroc 0.87\n",
      "\n",
      "epoch 49: trainloss 0.0581, valloss 0.5368, precision 0.68, recall 0.74, fscore 0.69, kappa 0.39, auroc 0.81\n",
      "\n",
      "epoch 50: trainloss 0.0590, valloss 0.3768, precision 0.73, recall 0.80, fscore 0.75, kappa 0.51, auroc 0.88\n",
      "\n",
      "[[ 0.1714  0.1055  0.1002  0.0975  0.0972  0.0932  0.0896  0.0842  0.0824\n",
      "   0.0823  0.0814  0.0784  0.0757  0.0763  0.073   0.0731  0.0698  0.0702\n",
      "   0.0684  0.0671  0.0674  0.067   0.0647  0.0668  0.0649  0.0651  0.0637\n",
      "   0.0637  0.0621  0.063   0.0625  0.0626  0.0621  0.0622  0.0604  0.0612\n",
      "   0.0613  0.0613  0.0603  0.0591  0.0599  0.0594  0.0589  0.0602  0.0588\n",
      "   0.0577  0.0596  0.0588  0.0581  0.059 ]\n",
      " [ 0.7717  0.4431  0.4785  0.4765  0.4372  0.4204  0.4867  0.4009  0.6025\n",
      "   0.3935  0.423   0.3999  0.4048  0.4111  0.362   0.3873  0.3737  0.3773\n",
      "   0.4191  0.4137  0.4437  0.4778  0.3719  0.401   0.4153  0.4549  0.3596\n",
      "   0.3897  0.3878  0.4018  0.4327  0.4285  0.3902  0.4301  0.3652  0.4013\n",
      "   0.4009  0.4198  0.4415  0.4459  0.4008  0.4566  0.463   0.4348  0.3682\n",
      "   0.4396  0.4105  0.4216  0.5368  0.3768]\n",
      " [ 0.38    0.86    0.86    0.86    0.85    0.82    0.81    0.81    0.72\n",
      "   0.73    0.78    0.7     0.75    0.7     0.77    0.72    0.75    0.73\n",
      "   0.71    0.72    0.71    0.7     0.74    0.71    0.71    0.69    0.76\n",
      "   0.72    0.78    0.72    0.7     0.71    0.72    0.7     0.74    0.72\n",
      "   0.74    0.72    0.71    0.71    0.71    0.7     0.71    0.7     0.76\n",
      "   0.71    0.73    0.73    0.68    0.73  ]\n",
      " [ 0.5     0.66    0.61    0.57    0.68    0.77    0.72    0.8     0.68\n",
      "   0.8     0.73    0.78    0.79    0.78    0.8     0.8     0.79    0.8\n",
      "   0.78    0.78    0.78    0.77    0.79    0.78    0.78    0.75    0.8\n",
      "   0.78    0.77    0.78    0.75    0.78    0.79    0.77    0.8     0.79\n",
      "   0.77    0.78    0.75    0.78    0.78    0.78    0.78    0.78    0.81\n",
      "   0.75    0.8     0.79    0.74    0.8   ]\n",
      " [ 0.43    0.69    0.63    0.56    0.71    0.79    0.74    0.81    0.7\n",
      "   0.74    0.75    0.7     0.77    0.68    0.78    0.72    0.76    0.75\n",
      "   0.71    0.73    0.71    0.71    0.76    0.71    0.72    0.7     0.77\n",
      "   0.73    0.78    0.74    0.72    0.72    0.73    0.7     0.76    0.73\n",
      "   0.75    0.73    0.72    0.71    0.72    0.7     0.71    0.7     0.77\n",
      "   0.72    0.74    0.74    0.69    0.75  ]\n",
      " [-0.      0.4     0.31    0.2     0.44    0.58    0.5     0.61    0.4\n",
      "   0.49    0.5     0.43    0.53    0.41    0.56    0.47    0.53    0.51\n",
      "   0.44    0.47    0.44    0.44    0.52    0.45    0.46    0.41    0.55\n",
      "   0.46    0.55    0.48    0.44    0.46    0.48    0.41    0.53    0.47\n",
      "   0.51    0.46    0.45    0.45    0.46    0.43    0.45    0.43    0.55\n",
      "   0.44    0.49    0.49    0.39    0.51  ]\n",
      " [ 0.87    0.88    0.89    0.9     0.9     0.89    0.86    0.9     0.78\n",
      "   0.89    0.88    0.87    0.87    0.87    0.89    0.87    0.87    0.87\n",
      "   0.85    0.86    0.85    0.85    0.87    0.87    0.86    0.83    0.89\n",
      "   0.86    0.88    0.87    0.84    0.86    0.87    0.85    0.89    0.86\n",
      "   0.87    0.87    0.85    0.86    0.86    0.86    0.86    0.88    0.89\n",
      "   0.84    0.88    0.87    0.81    0.88  ]]\n",
      "None\n",
      "epoch 1: trainloss 0.2341, valloss 0.0704, precision 0.55, recall 0.53, fscore 0.54, kappa 0.08, auroc 0.74\n",
      "\n",
      "epoch 2: trainloss 0.1542, valloss 0.0844, precision 0.54, recall 0.60, fscore 0.56, kappa 0.12, auroc 0.79\n",
      "\n",
      "epoch 3: trainloss 0.1402, valloss 0.0678, precision 0.55, recall 0.59, fscore 0.57, kappa 0.13, auroc 0.82\n",
      "\n",
      "epoch 4: trainloss 0.1343, valloss 0.0593, precision 0.57, recall 0.51, fscore 0.51, kappa 0.03, auroc 0.83\n",
      "\n",
      "epoch 5: trainloss 0.1306, valloss 0.0610, precision 0.55, recall 0.55, fscore 0.55, kappa 0.11, auroc 0.84\n",
      "\n",
      "epoch 6: trainloss 0.1264, valloss 0.0599, precision 0.57, recall 0.54, fscore 0.55, kappa 0.10, auroc 0.82\n",
      "\n",
      "epoch 7: trainloss 0.1266, valloss 0.0650, precision 0.56, recall 0.57, fscore 0.56, kappa 0.13, auroc 0.83\n",
      "\n",
      "epoch 8: trainloss 0.1221, valloss 0.0948, precision 0.56, recall 0.52, fscore 0.53, kappa 0.07, auroc 0.75\n",
      "\n",
      "epoch 9: trainloss 0.1209, valloss 0.0561, precision 0.56, recall 0.50, fscore 0.51, kappa 0.02, auroc 0.86\n",
      "\n",
      "epoch 10: trainloss 0.1193, valloss 0.0574, precision 0.56, recall 0.55, fscore 0.55, kappa 0.11, auroc 0.86\n",
      "\n",
      "epoch 11: trainloss 0.1174, valloss 0.0570, precision 0.57, recall 0.50, fscore 0.50, kappa 0.01, auroc 0.85\n",
      "\n",
      "epoch 12: trainloss 0.1180, valloss 0.0600, precision 0.56, recall 0.57, fscore 0.57, kappa 0.13, auroc 0.87\n",
      "\n",
      "epoch 13: trainloss 0.1148, valloss 0.0560, precision 0.58, recall 0.58, fscore 0.58, kappa 0.15, auroc 0.88\n",
      "\n",
      "epoch 14: trainloss 0.1147, valloss 0.0537, precision 0.60, recall 0.53, fscore 0.55, kappa 0.09, auroc 0.87\n",
      "\n",
      "epoch 15: trainloss 0.1138, valloss 0.0558, precision 0.58, recall 0.52, fscore 0.54, kappa 0.07, auroc 0.86\n",
      "\n",
      "epoch 16: trainloss 0.1127, valloss 0.0570, precision 0.58, recall 0.56, fscore 0.57, kappa 0.13, auroc 0.87\n",
      "\n",
      "epoch 17: trainloss 0.1120, valloss 0.0581, precision 0.57, recall 0.60, fscore 0.58, kappa 0.16, auroc 0.89\n",
      "\n",
      "epoch 18: trainloss 0.1115, valloss 0.0564, precision 0.59, recall 0.58, fscore 0.58, kappa 0.17, auroc 0.88\n",
      "\n",
      "epoch 19: trainloss 0.1115, valloss 0.0544, precision 0.58, recall 0.58, fscore 0.58, kappa 0.16, auroc 0.89\n",
      "\n",
      "epoch 20: trainloss 0.1094, valloss 0.0626, precision 0.57, recall 0.62, fscore 0.59, kappa 0.17, auroc 0.88\n",
      "\n",
      "epoch 21: trainloss 0.1102, valloss 0.0515, precision 0.60, recall 0.56, fscore 0.58, kappa 0.15, auroc 0.89\n",
      "\n",
      "epoch 22: trainloss 0.1074, valloss 0.0512, precision 0.59, recall 0.56, fscore 0.57, kappa 0.14, auroc 0.90\n",
      "\n",
      "epoch 23: trainloss 0.1081, valloss 0.0513, precision 0.62, recall 0.54, fscore 0.56, kappa 0.13, auroc 0.89\n",
      "\n",
      "epoch 24: trainloss 0.1077, valloss 0.0543, precision 0.60, recall 0.57, fscore 0.58, kappa 0.17, auroc 0.89\n",
      "\n",
      "epoch 25: trainloss 0.1081, valloss 0.0518, precision 0.60, recall 0.57, fscore 0.58, kappa 0.17, auroc 0.89\n",
      "\n",
      "epoch 26: trainloss 0.1081, valloss 0.0556, precision 0.59, recall 0.58, fscore 0.59, kappa 0.17, auroc 0.89\n",
      "\n",
      "epoch 27: trainloss 0.1078, valloss 0.0562, precision 0.58, recall 0.62, fscore 0.60, kappa 0.20, auroc 0.89\n",
      "\n",
      "epoch 28: trainloss 0.1054, valloss 0.0519, precision 0.63, recall 0.56, fscore 0.58, kappa 0.16, auroc 0.88\n",
      "\n",
      "epoch 29: trainloss 0.1063, valloss 0.0851, precision 0.60, recall 0.58, fscore 0.59, kappa 0.18, auroc 0.88\n",
      "\n",
      "epoch 30: trainloss 0.1061, valloss 0.0507, precision 0.63, recall 0.54, fscore 0.56, kappa 0.13, auroc 0.89\n",
      "\n",
      "epoch 31: trainloss 0.1034, valloss 0.0515, precision 0.62, recall 0.55, fscore 0.57, kappa 0.15, auroc 0.89\n",
      "\n",
      "epoch 32: trainloss 0.1051, valloss 0.0483, precision 0.64, recall 0.57, fscore 0.60, kappa 0.19, auroc 0.91\n",
      "\n",
      "epoch 33: trainloss 0.1044, valloss 0.0528, precision 0.60, recall 0.58, fscore 0.59, kappa 0.18, auroc 0.89\n",
      "\n",
      "epoch 34: trainloss 0.1050, valloss 0.0531, precision 0.60, recall 0.59, fscore 0.59, kappa 0.19, auroc 0.89\n",
      "\n",
      "epoch 35: trainloss 0.1033, valloss 0.0496, precision 0.66, recall 0.56, fscore 0.59, kappa 0.18, auroc 0.89\n",
      "\n",
      "epoch 36: trainloss 0.1051, valloss 0.0489, precision 0.69, recall 0.54, fscore 0.56, kappa 0.13, auroc 0.90\n",
      "\n",
      "epoch 37: trainloss 0.1036, valloss 0.0494, precision 0.64, recall 0.57, fscore 0.60, kappa 0.19, auroc 0.90\n",
      "\n",
      "epoch 38: trainloss 0.1029, valloss 0.0540, precision 0.59, recall 0.63, fscore 0.61, kappa 0.21, auroc 0.90\n",
      "\n",
      "epoch 39: trainloss 0.1025, valloss 0.0478, precision 0.65, recall 0.57, fscore 0.60, kappa 0.19, auroc 0.91\n",
      "\n",
      "[[0.2341 0.1542 0.1402 0.1343 0.1306 0.1264 0.1266 0.1221 0.1209 0.1193\n",
      "  0.1174 0.118  0.1148 0.1147 0.1138 0.1127 0.112  0.1115 0.1115 0.1094\n",
      "  0.1102 0.1074 0.1081 0.1077 0.1081 0.1081 0.1078 0.1054 0.1063 0.1061\n",
      "  0.1034 0.1051 0.1044 0.105  0.1033 0.1051 0.1036 0.1029 0.1025 0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.0704 0.0844 0.0678 0.0593 0.061  0.0599 0.065  0.0948 0.0561 0.0574\n",
      "  0.057  0.06   0.056  0.0537 0.0558 0.057  0.0581 0.0564 0.0544 0.0626\n",
      "  0.0515 0.0512 0.0513 0.0543 0.0518 0.0556 0.0562 0.0519 0.0851 0.0507\n",
      "  0.0515 0.0483 0.0528 0.0531 0.0496 0.0489 0.0494 0.054  0.0478 0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.55   0.54   0.55   0.57   0.55   0.57   0.56   0.56   0.56   0.56\n",
      "  0.57   0.56   0.58   0.6    0.58   0.58   0.57   0.59   0.58   0.57\n",
      "  0.6    0.59   0.62   0.6    0.6    0.59   0.58   0.63   0.6    0.63\n",
      "  0.62   0.64   0.6    0.6    0.66   0.69   0.64   0.59   0.65   0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.53   0.6    0.59   0.51   0.55   0.54   0.57   0.52   0.5    0.55\n",
      "  0.5    0.57   0.58   0.53   0.52   0.56   0.6    0.58   0.58   0.62\n",
      "  0.56   0.56   0.54   0.57   0.57   0.58   0.62   0.56   0.58   0.54\n",
      "  0.55   0.57   0.58   0.59   0.56   0.54   0.57   0.63   0.57   0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.54   0.56   0.57   0.51   0.55   0.55   0.56   0.53   0.51   0.55\n",
      "  0.5    0.57   0.58   0.55   0.54   0.57   0.58   0.58   0.58   0.59\n",
      "  0.58   0.57   0.56   0.58   0.58   0.59   0.6    0.58   0.59   0.56\n",
      "  0.57   0.6    0.59   0.59   0.59   0.56   0.6    0.61   0.6    0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.08   0.12   0.13   0.03   0.11   0.1    0.13   0.07   0.02   0.11\n",
      "  0.01   0.13   0.15   0.09   0.07   0.13   0.16   0.17   0.16   0.17\n",
      "  0.15   0.14   0.13   0.17   0.17   0.17   0.2    0.16   0.18   0.13\n",
      "  0.15   0.19   0.18   0.19   0.18   0.13   0.19   0.21   0.19   0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.74   0.79   0.82   0.83   0.84   0.82   0.83   0.75   0.86   0.86\n",
      "  0.85   0.87   0.88   0.87   0.86   0.87   0.89   0.88   0.89   0.88\n",
      "  0.89   0.9    0.89   0.89   0.89   0.89   0.89   0.88   0.88   0.89\n",
      "  0.89   0.91   0.89   0.89   0.89   0.9    0.9    0.9    0.91   0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.    ]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2591064/1533990215.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
      "  trainloss.plot(range(50), vals.sum(axis=0)[0] / counts,label=path)\n",
      "/tmp/ipykernel_2591064/1533990215.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
      "  valloss.plot(range(50), vals.sum(axis=0)[1] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision.plot(range(50), vals.sum(axis=0)[2] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall.plot(range(50), vals.sum(axis=0)[3] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:51: RuntimeWarning: invalid value encountered in true_divide\n",
      "  fscore.plot(range(50), vals.sum(axis=0)[4] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:52: RuntimeWarning: invalid value encountered in true_divide\n",
      "  kappa.plot(range(50), vals.sum(axis=0)[5] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
      "  trainloss.plot(range(50), vals.sum(axis=0)[0] / counts,label=path)\n",
      "/tmp/ipykernel_2591064/1533990215.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
      "  valloss.plot(range(50), vals.sum(axis=0)[1] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision.plot(range(50), vals.sum(axis=0)[2] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall.plot(range(50), vals.sum(axis=0)[3] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:51: RuntimeWarning: invalid value encountered in true_divide\n",
      "  fscore.plot(range(50), vals.sum(axis=0)[4] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:52: RuntimeWarning: invalid value encountered in true_divide\n",
      "  kappa.plot(range(50), vals.sum(axis=0)[5] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
      "  trainloss.plot(range(50), vals.sum(axis=0)[0] / counts,label=path)\n",
      "/tmp/ipykernel_2591064/1533990215.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
      "  valloss.plot(range(50), vals.sum(axis=0)[1] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision.plot(range(50), vals.sum(axis=0)[2] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall.plot(range(50), vals.sum(axis=0)[3] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:51: RuntimeWarning: invalid value encountered in true_divide\n",
      "  fscore.plot(range(50), vals.sum(axis=0)[4] / counts)\n",
      "/tmp/ipykernel_2591064/1533990215.py:52: RuntimeWarning: invalid value encountered in true_divide\n",
      "  kappa.plot(range(50), vals.sum(axis=0)[5] / counts)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f296842fc70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "os.listdir(\".\")\n",
    "fig, [[trainloss, valloss, precision], [recall, fscore, kappa]] = plt.subplots(nrows=2, ncols=3, figsize=(15, 6))\n",
    "trainloss.set_title(\"Train Loss\")\n",
    "valloss.set_title(\"Validation Loss\")\n",
    "precision.set_title(\"Precision\")\n",
    "recall.set_title(\"Recall\")\n",
    "fscore.set_title(\"Fscore\")\n",
    "kappa.set_title(\"Kappa\")\n",
    "\n",
    "def read_log(logname):\n",
    "       \n",
    "    lines = open(logname).readlines()\n",
    "    vals = np.zeros((5, 7, 50))\n",
    "    \n",
    "    for i in range(0, len(logs), 50):\n",
    "        currentlines = lines[i:i+50]\n",
    "        \n",
    "        for j, line in enumerate(currentlines):\n",
    "            print(line)\n",
    "            vals[i//50][0][j] = float(line.split(\"trainloss \")[1].split(\", \")[0])\n",
    "            vals[i//50][1][j] = float(line.split(\"valloss \")[1].split(\", \")[0])\n",
    "            vals[i//50][2][j] = float(line.split(\"precision \")[1].split(\", \")[0])\n",
    "            vals[i//50][3][j] = float(line.split(\"recall \")[1].split(\", \")[0])\n",
    "            vals[i//50][4][j] = float(line.split(\"fscore \")[1].split(\", \")[0])\n",
    "            vals[i//50][5][j] = float(line.split(\"kappa \")[1].split(\", \")[0])\n",
    "            vals[i//50][6][j] = float(line.split(\"auroc \")[1].split(\" \")[0])\n",
    "    return vals\n",
    "\n",
    "for path in ['models_untrained_unet', \"models_untrained_resunet\", \"models_imagenet_resunet\",]:\n",
    "    \n",
    "    vals = np.zeros((5, 6, 50))\n",
    "    log = read_log(path + \"log.txt\")\n",
    "    print(log)\n",
    "    counts = np.zeros(50)\n",
    "    #for i in range(2):\n",
    "    #    counts[:len(loaded[i])] += 1\n",
    "    #    print(path,i, len(loaded[i]))\n",
    "    #    for j, field in enumerate([\"trainloss\", 'valloss', 'precision', 'recall', 'fscore', 'kappa']):\n",
    "    #        vals[i][j][:len(loaded[i])] = [k[field] for k in loaded[i]]\n",
    "    \n",
    "    trainloss.plot(range(50), vals.sum(axis=0)[0] / counts,label=path)\n",
    "    valloss.plot(range(50), vals.sum(axis=0)[1] / counts)\n",
    "    precision.plot(range(50), vals.sum(axis=0)[2] / counts)\n",
    "    recall.plot(range(50), vals.sum(axis=0)[3] / counts)\n",
    "    fscore.plot(range(50), vals.sum(axis=0)[4] / counts)\n",
    "    kappa.plot(range(50), vals.sum(axis=0)[5] / counts)\n",
    "trainloss.legend()\n",
    "#plt.legend()%   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f93b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
